Logging to: logs/run.log
=== Starting pipeline at Sun Dec 14 19:24:22 UTC 2025 ===
Running script: 00_download_data.py
Downloading to /app/data/downloaded.zip ...
File downloaded successfully!
Running script: 01_data_preprocessing.py
ZIP extracted to: /app/extracted_zip
ERROR loading ujvbty.json: 'rating'
ERROR loading meta.json: 'str' object has no attribute 'get'
Base DF columns: Index(['text', 'rating'], dtype='object')
Consensus DF columns: Index(['text', 'rating'], dtype='object')
Base DF shape: (3737, 2)
Consensus DF shape: (2648, 2)
Number of rows filtered out from base_df because they exist in consensus: 615
Number of aggregated consensus texts (after averaging): 609
Consensus inference saved to: /app/data/inference.csv
Train/Val/Test split saved.
Running script: 02_train.py
[2025-12-14 19:24:58,163] INFO: 

==== FULL DATASET TRAIN/VAL START ====
[2025-12-14 19:25:11,276] INFO: Using device: cuda
Alkalmazott jogi augmentáció...
Augmentáció kész!
[2025-12-14 19:25:11,443] INFO: Train size: 2434, Val size: 272
[2025-12-14 19:25:11,485] INFO: collecting all words and their counts
[2025-12-14 19:25:11,485] INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
[2025-12-14 19:25:11,524] INFO: collected 23839 word types from a corpus of 126030 raw words and 2434 sentences
[2025-12-14 19:25:11,525] INFO: Creating a fresh vocabulary
[2025-12-14 19:25:11,692] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 23839 unique words (100.00% of original 23839, drops 0)', 'datetime': '2025-12-14T19:25:11.683959', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-14 19:25:11,692] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 126030 word corpus (100.00% of original 126030, drops 0)', 'datetime': '2025-12-14T19:25:11.692809', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-14 19:25:11,939] INFO: deleting the raw counts dictionary of 23839 items
[2025-12-14 19:25:11,941] INFO: sample=0.001 downsamples 22 most-common words
[2025-12-14 19:25:11,942] INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 104660.80980236958 word corpus (83.0%% of prior 126030)', 'datetime': '2025-12-14T19:25:11.942064', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-14 19:25:12,329] INFO: estimated required memory for 23839 words and 64 dimensions: 24125068 bytes
[2025-12-14 19:25:12,329] INFO: resetting layer weights
[2025-12-14 19:25:12,352] INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-14T19:25:12.352062', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}
[2025-12-14 19:25:12,352] INFO: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 23839 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-14T19:25:12.352449', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-14 19:25:12,489] INFO: EPOCH 0: training on 126030 raw words (104747 effective words) took 0.1s, 815046 effective words/s
[2025-12-14 19:25:12,614] INFO: EPOCH 1: training on 126030 raw words (104641 effective words) took 0.1s, 874876 effective words/s
[2025-12-14 19:25:12,736] INFO: EPOCH 2: training on 126030 raw words (104681 effective words) took 0.1s, 926815 effective words/s
[2025-12-14 19:25:12,868] INFO: EPOCH 3: training on 126030 raw words (104667 effective words) took 0.1s, 834319 effective words/s
[2025-12-14 19:25:12,991] INFO: EPOCH 4: training on 126030 raw words (104688 effective words) took 0.1s, 900840 effective words/s
[2025-12-14 19:25:12,991] INFO: Word2Vec lifecycle event {'msg': 'training on 630150 raw words (523424 effective words) took 0.6s, 819333 effective words/s', 'datetime': '2025-12-14T19:25:12.991468', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-14 19:25:12,991] INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=23839, vector_size=64, alpha=0.025>', 'datetime': '2025-12-14T19:25:12.991657', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}
[2025-12-14 19:25:12,991] INFO: Word2Vec trained. Vocab size: 23839 Embedding dim: 64
[2025-12-14 19:25:13,363] INFO: ==== CONFIG & HYPERPARAMETERS START ====
[2025-12-14 19:25:13,363] INFO: AUGMENTATION        : True
[2025-12-14 19:25:13,363] INFO: AUG_PROB            : 0.15
[2025-12-14 19:25:13,363] INFO: BATCH_SIZE          : 32
[2025-12-14 19:25:13,363] INFO: BIDIRECTIONAL       : False
[2025-12-14 19:25:13,363] INFO: DATA_DIR            : /app/data
[2025-12-14 19:25:13,363] INFO: EARLY_STOP_PATIENCE : 20
[2025-12-14 19:25:13,364] INFO: FC_DROPOUT          : 0.4
[2025-12-14 19:25:13,364] INFO: LEARNING_RATE       : 0.001
[2025-12-14 19:25:13,364] INFO: LOG_FILE            : /app/logs/run.log
[2025-12-14 19:25:13,364] INFO: LSTM_DROPOUT        : 0.3
[2025-12-14 19:25:13,364] INFO: LSTM_HIDDEN_DIM     : 128
[2025-12-14 19:25:13,365] INFO: MODEL_NAME          : LSTM
[2025-12-14 19:25:13,365] INFO: MODEL_PATH          : /app/output/lstm_model.pth
[2025-12-14 19:25:13,365] INFO: NUM_CLASSES         : 5
[2025-12-14 19:25:13,365] INFO: NUM_EPOCHS          : 1000
[2025-12-14 19:25:13,366] INFO: NUM_LSTM_LAYERS     : 2
[2025-12-14 19:25:13,366] INFO: OUTPUT_DIR          : /app/output
[2025-12-14 19:25:13,366] INFO: SEQ_LEN             : 500
[2025-12-14 19:25:13,366] INFO: W2V_PATH            : /app/output/w2v.model
[2025-12-14 19:25:13,367] INFO: WEIGHT_DECAY        : 0.0001
[2025-12-14 19:25:13,367] INFO: WORD2_VEC_MIN_COUNT : 1
[2025-12-14 19:25:13,367] INFO: WORD2_VEC_VECTOR_SIZE: 64
[2025-12-14 19:25:13,367] INFO: ==== CONFIG & HYPERPARAMETERS END ====
[2025-12-14 19:25:27,561] INFO: ==== MODEL SUMMARY START ====
[2025-12-14 19:25:27,561] INFO: Model architecture:
[2025-12-14 19:25:27,561] INFO: MultiLayerLSTM(
  (emb): Embedding(23840, 64, padding_idx=0)
  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)
  (attention): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.4, inplace=False)
  (fc): Linear(in_features=128, out_features=5, bias=True)
)
[2025-12-14 19:25:27,562] INFO: Total parameters      : 1,757,958
[2025-12-14 19:25:27,562] INFO: Trainable parameters  : 1,757,958
[2025-12-14 19:25:27,562] INFO: Frozen parameters     : 0
[2025-12-14 19:25:27,563] INFO: Embedding parameters  : 1,525,760
[2025-12-14 19:25:27,563] INFO: ==== MODEL SUMMARY END ====
[2025-12-14 19:25:38,034] INFO: Epoch 1/1000 | Train Loss: 1.3842 | Train Acc: 36.07% | Val Loss: 1.3642 | Val Acc: 38.24%
[2025-12-14 19:25:38,070] INFO: Validation loss improved. Model saved to /app/output/lstm_model.pth
[2025-12-14 19:25:42,515] INFO: Epoch 2/1000 | Train Loss: 1.2943 | Train Acc: 43.80% | Val Loss: 1.3491 | Val Acc: 38.97%
[2025-12-14 19:25:42,539] INFO: Validation loss improved. Model saved to /app/output/lstm_model.pth
[2025-12-14 19:25:46,978] INFO: Epoch 3/1000 | Train Loss: 1.2864 | Train Acc: 44.08% | Val Loss: 1.3326 | Val Acc: 38.97%
[2025-12-14 19:25:47,002] INFO: Validation loss improved. Model saved to /app/output/lstm_model.pth
[2025-12-14 19:25:51,468] INFO: Epoch 4/1000 | Train Loss: 1.2569 | Train Acc: 45.65% | Val Loss: 1.3770 | Val Acc: 38.60%
[2025-12-14 19:25:51,468] INFO: No improvement in val loss for 1 epoch(s).
[2025-12-14 19:25:55,925] INFO: Epoch 5/1000 | Train Loss: 1.0796 | Train Acc: 53.04% | Val Loss: 1.4114 | Val Acc: 38.24%
[2025-12-14 19:25:55,926] INFO: No improvement in val loss for 2 epoch(s).
[2025-12-14 19:26:00,381] INFO: Epoch 6/1000 | Train Loss: 0.8420 | Train Acc: 66.52% | Val Loss: 1.6610 | Val Acc: 39.71%
[2025-12-14 19:26:00,381] INFO: No improvement in val loss for 3 epoch(s).
[2025-12-14 19:26:04,862] INFO: Epoch 7/1000 | Train Loss: 0.7214 | Train Acc: 71.32% | Val Loss: 1.8481 | Val Acc: 37.50%
[2025-12-14 19:26:04,862] INFO: No improvement in val loss for 4 epoch(s).
[2025-12-14 19:26:09,353] INFO: Epoch 8/1000 | Train Loss: 0.6298 | Train Acc: 75.64% | Val Loss: 2.1890 | Val Acc: 38.24%
[2025-12-14 19:26:09,353] INFO: No improvement in val loss for 5 epoch(s).
[2025-12-14 19:26:13,799] INFO: Epoch 9/1000 | Train Loss: 0.5497 | Train Acc: 79.25% | Val Loss: 2.1735 | Val Acc: 37.13%
[2025-12-14 19:26:13,799] INFO: No improvement in val loss for 6 epoch(s).
[2025-12-14 19:26:18,292] INFO: Epoch 10/1000 | Train Loss: 0.4680 | Train Acc: 84.43% | Val Loss: 2.4329 | Val Acc: 38.24%
[2025-12-14 19:26:18,292] INFO: No improvement in val loss for 7 epoch(s).
[2025-12-14 19:26:22,862] INFO: Epoch 11/1000 | Train Loss: 0.3806 | Train Acc: 87.63% | Val Loss: 2.6625 | Val Acc: 38.24%
[2025-12-14 19:26:22,862] INFO: No improvement in val loss for 8 epoch(s).
[2025-12-14 19:26:27,375] INFO: Epoch 12/1000 | Train Loss: 0.3273 | Train Acc: 89.28% | Val Loss: 2.9508 | Val Acc: 37.13%
[2025-12-14 19:26:27,375] INFO: No improvement in val loss for 9 epoch(s).
[2025-12-14 19:26:31,986] INFO: Epoch 13/1000 | Train Loss: 0.2934 | Train Acc: 90.14% | Val Loss: 3.2010 | Val Acc: 35.29%
[2025-12-14 19:26:31,987] INFO: No improvement in val loss for 10 epoch(s).
[2025-12-14 19:26:36,567] INFO: Epoch 14/1000 | Train Loss: 0.2566 | Train Acc: 91.99% | Val Loss: 3.3697 | Val Acc: 34.56%
[2025-12-14 19:26:36,567] INFO: No improvement in val loss for 11 epoch(s).
[2025-12-14 19:26:41,128] INFO: Epoch 15/1000 | Train Loss: 0.2164 | Train Acc: 92.89% | Val Loss: 3.5878 | Val Acc: 34.56%
[2025-12-14 19:26:41,129] INFO: No improvement in val loss for 12 epoch(s).
[2025-12-14 19:26:45,651] INFO: Epoch 16/1000 | Train Loss: 0.2029 | Train Acc: 92.69% | Val Loss: 3.7919 | Val Acc: 33.46%
[2025-12-14 19:26:45,652] INFO: No improvement in val loss for 13 epoch(s).
[2025-12-14 19:26:50,264] INFO: Epoch 17/1000 | Train Loss: 0.1940 | Train Acc: 93.26% | Val Loss: 3.8576 | Val Acc: 33.09%
[2025-12-14 19:26:50,264] INFO: No improvement in val loss for 14 epoch(s).
[2025-12-14 19:26:54,848] INFO: Epoch 18/1000 | Train Loss: 0.2112 | Train Acc: 93.18% | Val Loss: 3.8640 | Val Acc: 34.93%
[2025-12-14 19:26:54,848] INFO: No improvement in val loss for 15 epoch(s).
[2025-12-14 19:26:59,405] INFO: Epoch 19/1000 | Train Loss: 0.1657 | Train Acc: 95.28% | Val Loss: 3.7371 | Val Acc: 36.40%
[2025-12-14 19:26:59,406] INFO: No improvement in val loss for 16 epoch(s).
[2025-12-14 19:27:04,058] INFO: Epoch 20/1000 | Train Loss: 0.1347 | Train Acc: 96.80% | Val Loss: 3.9968 | Val Acc: 34.93%
[2025-12-14 19:27:04,059] INFO: No improvement in val loss for 17 epoch(s).
[2025-12-14 19:27:08,592] INFO: Epoch 21/1000 | Train Loss: 0.1051 | Train Acc: 97.08% | Val Loss: 4.5242 | Val Acc: 33.82%
[2025-12-14 19:27:08,592] INFO: No improvement in val loss for 18 epoch(s).
[2025-12-14 19:27:13,265] INFO: Epoch 22/1000 | Train Loss: 0.0890 | Train Acc: 97.45% | Val Loss: 4.1995 | Val Acc: 37.87%
[2025-12-14 19:27:13,266] INFO: No improvement in val loss for 19 epoch(s).
[2025-12-14 19:27:17,820] INFO: Epoch 23/1000 | Train Loss: 0.1121 | Train Acc: 97.37% | Val Loss: 4.6614 | Val Acc: 34.56%
[2025-12-14 19:27:17,820] INFO: No improvement in val loss for 20 epoch(s).
[2025-12-14 19:27:17,820] INFO: Early stopping triggered after 20 epochs with no improvement.
[2025-12-14 19:27:17,821] INFO: Word2Vec lifecycle event {'fname_or_handle': '/app/output/w2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-12-14T19:27:17.821160', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'saving'}
[2025-12-14 19:27:17,821] INFO: not storing attribute cum_table
[2025-12-14 19:27:17,881] INFO: saved /app/output/w2v.model
[2025-12-14 19:27:17,881] INFO: Word2Vec model saved to /app/data/w2v.model
[2025-12-14 19:27:17,881] INFO: ==== FULL DATASET TRAIN/VAL END ====
Running script: 03_1_evaluation_test_set.py
[2025-12-14 19:27:25,756] INFO: 

==== EVALUATION START ====
[2025-12-14 19:27:28,036] INFO: Using device: cuda
[2025-12-14 19:27:28,129] INFO: GPU: NVIDIA GeForce GTX 1050 Ti, CUDA capability: 6.1
[2025-12-14 19:27:28,155] INFO: Loaded test dataset with 307 samples.
[2025-12-14 19:27:28,159] INFO: loading Word2Vec object from /app/output/w2v.model
[2025-12-14 19:27:28,244] INFO: loading wv recursively from /app/output/w2v.model.wv.* with mmap=None
[2025-12-14 19:27:28,245] INFO: setting ignored attribute cum_table to None
[2025-12-14 19:27:28,629] INFO: Word2Vec lifecycle event {'fname': '/app/output/w2v.model', 'datetime': '2025-12-14T19:27:28.626848', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-14 19:27:28,629] INFO: Word2Vec model loaded from /app/output/w2v.model (vocab size=23839, vector size=64)
[2025-12-14 19:27:35,012] INFO: LSTM model loaded from /app/output/lstm_model.pth and ready for evaluation.
[2025-12-14 19:27:35,012] INFO: Starting LSTM evaluation on test set...
[2025-12-14 19:27:35,785] INFO: LSTM Test Loss: 1.3459
[2025-12-14 19:27:35,785] INFO: LSTM Test Accuracy: 42.35%
[2025-12-14 19:27:35,785] INFO: LSTM F1-score (weighted): 0.3754
[2025-12-14 19:27:35,786] INFO: LSTM Confusion Matrix:
[[ 0  0  5  4  4]
 [ 0  2  8 16  9]
 [ 0  3  9 36 17]
 [ 0  0 13 58 35]
 [ 0  0  1 26 61]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-14 19:27:35,870] INFO: LSTM Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.40      0.06      0.10        35
           2       0.25      0.14      0.18        65
           3       0.41      0.55      0.47       106
           4       0.48      0.69      0.57        88

    accuracy                           0.42       307
   macro avg       0.31      0.29      0.26       307
weighted avg       0.38      0.42      0.38       307

[2025-12-14 19:27:35,870] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-14 19:27:35,870] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-14 19:27:35,925] INFO: Train dataset loaded: 2434 samples.
[2025-12-14 19:27:37,984] INFO: TF-IDF + Logistic Regression trained.
[2025-12-14 19:27:38,026] INFO: TF-IDF + Logistic Regression Test Accuracy: 36.81%
[2025-12-14 19:27:38,027] INFO: TF-IDF + Logistic Regression F1-score (weighted): 0.3350
[2025-12-14 19:27:38,027] INFO: TF-IDF + Logistic Regression Confusion Matrix:
[[ 0  2  4  2  5]
 [ 0  1 16 14  4]
 [ 0  4  9 33 19]
 [ 0  1 24 47 34]
 [ 0  0  8 24 56]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-14 19:27:38,045] INFO: TF-IDF Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.12      0.03      0.05        35
           2       0.15      0.14      0.14        65
           3       0.39      0.44      0.42       106
           4       0.47      0.64      0.54        88

    accuracy                           0.37       307
   macro avg       0.23      0.25      0.23       307
weighted avg       0.32      0.37      0.34       307

[2025-12-14 19:27:38,045] INFO: ==== EVALUATION END ====
Running script: 03_2_evaluation_consensus.py
[2025-12-14 19:27:43,155] INFO: 

==== EVALUATION CONSENSUS ====
[2025-12-14 19:27:45,270] INFO: Using device: cuda
[2025-12-14 19:27:45,301] INFO: Loaded 595 samples from /app/data/inference.csv
[2025-12-14 19:27:45,316] INFO: loading Word2Vec object from /app/output/w2v.model
[2025-12-14 19:27:45,376] INFO: loading wv recursively from /app/output/w2v.model.wv.* with mmap=None
[2025-12-14 19:27:45,377] INFO: setting ignored attribute cum_table to None
[2025-12-14 19:27:45,793] INFO: Word2Vec lifecycle event {'fname': '/app/output/w2v.model', 'datetime': '2025-12-14T19:27:45.790945', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-14 19:27:45,793] INFO: Word2Vec loaded from /app/output/w2v.model (vocab size=23839, vector size=64)
[2025-12-14 19:27:45,793] INFO: Word2Vec loaded: vocab size=23839, vector size=64
[2025-12-14 19:27:52,180] INFO: LSTM model loaded successfully from /app/output/lstm_model.pth
[2025-12-14 19:27:52,180] INFO: Starting LSTM evaluation on CONSENSUS...
[2025-12-14 19:27:52,978] INFO: LSTM CONSENSUS Loss: 1.5002
[2025-12-14 19:27:52,978] INFO: LSTM CONSENSUS Accuracy: 27.06%
[2025-12-14 19:27:52,978] INFO: LSTM F1-score (weighted): 0.2181
[2025-12-14 19:27:52,978] INFO: LSTM Confusion Matrix:
[[ 0  3 13 14  7]
 [ 0  2 16 56 17]
 [ 0 10  7 87 46]
 [ 0  7 28 88 96]
 [ 0  4  5 25 64]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-14 19:27:52,997] INFO: LSTM Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        37
           1       0.08      0.02      0.03        91
           2       0.10      0.05      0.06       150
           3       0.33      0.40      0.36       219
           4       0.28      0.65      0.39        98

    accuracy                           0.27       595
   macro avg       0.16      0.22      0.17       595
weighted avg       0.20      0.27      0.22       595

[2025-12-14 19:27:52,997] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-14 19:27:53,052] INFO: Loaded 2434 training samples for TF-IDF baseline.
[2025-12-14 19:27:55,609] INFO: TF-IDF + Logistic Regression trained.
[2025-12-14 19:27:55,690] INFO: CONSENSUS: TF-IDF + Logistic Regression Accuracy: 34.79%
[2025-12-14 19:27:55,690] INFO: CONSENSUS: TF-IDF + Logistic Regression F1-score (weighted): 0.3096
[2025-12-14 19:27:55,691] INFO: CONSENSUS_ TF-IDF Confusion Matrix:
[[  2   1   8  16  10]
 [  1   7  18  47  18]
 [  0   6  24  81  39]
 [  0   3  35 113  68]
 [  0   1  12  24  61]]
[2025-12-14 19:27:55,708] INFO: CONSENSUS: TF-IDF Classification Report:
              precision    recall  f1-score   support

           0       0.67      0.05      0.10        37
           1       0.39      0.08      0.13        91
           2       0.25      0.16      0.19       150
           3       0.40      0.52      0.45       219
           4       0.31      0.62      0.41        98

    accuracy                           0.35       595
   macro avg       0.40      0.29      0.26       595
weighted avg       0.36      0.35      0.31       595

[2025-12-14 19:27:55,708] INFO: ==== EVALUATION CONSENSUS END ====
Running script: 04_inference.py
[2025-12-14 19:28:00,879] INFO: 

==== INFERENCE START ====
[2025-12-14 19:28:02,930] INFO: Using device: cuda
[2025-12-14 19:28:02,958] INFO: Loaded 609 inference samples
[2025-12-14 19:28:02,970] INFO: loading Word2Vec object from /app/output/w2v.model
[2025-12-14 19:28:03,019] INFO: loading wv recursively from /app/output/w2v.model.wv.* with mmap=None
[2025-12-14 19:28:03,021] INFO: setting ignored attribute cum_table to None
[2025-12-14 19:28:03,417] INFO: Word2Vec lifecycle event {'fname': '/app/output/w2v.model', 'datetime': '2025-12-14T19:28:03.415574', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-14 19:28:03,418] INFO: Word2Vec loaded from /app/output/w2v.model (vocab size=23839, vector size=64)
[2025-12-14 19:28:09,415] INFO: Model loaded successfully
[2025-12-14 19:28:10,163] INFO: Example inferences:                                                 text    rating  predicted_label
0  1. Az OTP Bank Nyrt. az egyes Ügyfeleiről rend...  3.235294                4
1  1. Az OTP Bank Nyrt. jogosult meggyőződni az Ü...  4.421053                5
2  1. Az OTP Bank Nyrt. és az Ügyfél szerződéses ...  3.150000                4
3      1. SZOLGÁLTATÓ ELÉRHETŐSÉGE, ÁLTALÁNOS ADATOK       NaN                5
4           1. Általános rendelkezések, ÁSZF hatálya  5.000000                5
5  1.1. Az Általános Szerződési Feltételek (a tov...  3.000000                4
6  1.1. Szolgáltató neve, székhelyének postai cím...  4.000000                5
7  1.1.1. A Weboldalon történő Megrendeléshez a F...  5.000000                5
8  1.1.2. Amennyiben a Felhasználó Saját fiók has...  4.000000                5
9  1.2. A jelen ÁSZF hatálya alá tartozik a Ptk. ...  4.000000                5
[2025-12-14 19:28:10,197] INFO: Inference results saved to /app/data/inference_results.csv
[2025-12-14 19:28:10,198] INFO: ==== INFERENCE END ====
=== Finished at Sun Dec 14 19:28:11 UTC 2025 ===
