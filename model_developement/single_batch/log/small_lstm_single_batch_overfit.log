=== Starting pipeline at Sat Dec 13 12:06:44 UTC 2025 ===
Running script: 00_download_data.py
Downloading to /app/data/downloaded.zip ...
File downloaded successfully!
Running script: 01_data_preprocessing.py
ZIP extracted to: /app/extracted_zip
ERROR loading file.json: 'rating'
ERROR loading meta.json: 'str' object has no attribute 'get'
Base DF columns: Index(['text', 'rating'], dtype='object')
Consensus DF columns: Index(['text', 'rating'], dtype='object')
Base DF shape: (3737, 2)
Consensus DF shape: (2648, 2)
Number of rows filtered out from base_df because they exist in consensus: 615
Number of aggregated consensus texts (after averaging): 609
Consensus inference saved to: /app/data/inference.csv
Train/Val/Test split saved.
Running script: 02_train.py
[2025-12-13 12:06:56,534] INFO: ==== SINGLE BATCH OVERFIT START ====
[2025-12-13 12:06:59,188] INFO: Using device: cuda
[2025-12-13 12:06:59,234] INFO: Loaded 32 samples for overfit test
[2025-12-13 12:06:59,235] INFO: collecting all words and their counts
[2025-12-13 12:06:59,235] INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
[2025-12-13 12:06:59,236] INFO: collected 233 word types from a corpus of 314 raw words and 32 sentences
[2025-12-13 12:06:59,239] INFO: Creating a fresh vocabulary
[2025-12-13 12:06:59,248] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 233 unique words (100.00% of original 233, drops 0)', 'datetime': '2025-12-13T12:06:59.245657', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 12:06:59,248] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 314 word corpus (100.00% of original 314, drops 0)', 'datetime': '2025-12-13T12:06:59.248697', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 12:06:59,252] INFO: deleting the raw counts dictionary of 233 items
[2025-12-13 12:06:59,253] INFO: sample=0.001 downsamples 233 most-common words
[2025-12-13 12:06:59,253] INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 216.13118193396107 word corpus (68.8%% of prior 314)', 'datetime': '2025-12-13T12:06:59.253289', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 12:06:59,259] INFO: estimated required memory for 233 words and 64 dimensions: 235796 bytes
[2025-12-13 12:06:59,259] INFO: resetting layer weights
[2025-12-13 12:06:59,260] INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-13T12:06:59.260803', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}
[2025-12-13 12:06:59,261] INFO: Word2Vec lifecycle event {'msg': 'training model with 3 workers on 233 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-13T12:06:59.261142', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 12:06:59,267] INFO: EPOCH 0: training on 314 raw words (216 effective words) took 0.0s, 135244 effective words/s
[2025-12-13 12:06:59,272] INFO: EPOCH 1: training on 314 raw words (207 effective words) took 0.0s, 111152 effective words/s
[2025-12-13 12:06:59,277] INFO: EPOCH 2: training on 314 raw words (218 effective words) took 0.0s, 318851 effective words/s
[2025-12-13 12:06:59,282] INFO: EPOCH 3: training on 314 raw words (215 effective words) took 0.0s, 155909 effective words/s
[2025-12-13 12:06:59,287] INFO: EPOCH 4: training on 314 raw words (222 effective words) took 0.0s, 164358 effective words/s
[2025-12-13 12:06:59,287] INFO: Word2Vec lifecycle event {'msg': 'training on 1570 raw words (1078 effective words) took 0.0s, 41263 effective words/s', 'datetime': '2025-12-13T12:06:59.287559', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 12:06:59,287] INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=233, vector_size=64, alpha=0.025>', 'datetime': '2025-12-13T12:06:59.287689', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}
[2025-12-13 12:06:59,290] INFO: ==== CONFIG & HYPERPARAMETERS START ====
[2025-12-13 12:06:59,291] INFO: BATCH_SIZE          : 32
[2025-12-13 12:06:59,291] INFO: DATA_DIR            : /app/data
[2025-12-13 12:06:59,291] INFO: EARLY_STOP_PATIENCE : 40
[2025-12-13 12:06:59,291] INFO: FC_DROPOUT          : 0
[2025-12-13 12:06:59,291] INFO: LEARNING_RATE       : 0.04
[2025-12-13 12:06:59,291] INFO: LOG_FILE            : /app/logs/run.log
[2025-12-13 12:06:59,291] INFO: LSTM_DROPOUT        : 0
[2025-12-13 12:06:59,292] INFO: LSTM_HIDDEN_DIM     : 8
[2025-12-13 12:06:59,292] INFO: MODEL_NAME          : LSTM
[2025-12-13 12:06:59,292] INFO: MODEL_PATH          : /app/data/lstm_model.pth
[2025-12-13 12:06:59,292] INFO: NUM_CLASSES         : 5
[2025-12-13 12:06:59,292] INFO: NUM_EPOCHS          : 1000
[2025-12-13 12:06:59,292] INFO: NUM_LSTM_LAYERS     : 1
[2025-12-13 12:06:59,292] INFO: SEQ_LEN             : 10
[2025-12-13 12:06:59,293] INFO: W2V_PATH            : /app/data/w2v.model
[2025-12-13 12:06:59,293] INFO: WORD2_VEC_MIN_COUNT : 1
[2025-12-13 12:06:59,293] INFO: WORD2_VEC_VECTOR_SIZE: 64
[2025-12-13 12:06:59,293] INFO: ==== CONFIG & HYPERPARAMETERS END ====
[2025-12-13 12:07:05,498] INFO: ==== MODEL SUMMARY START ====
[2025-12-13 12:07:05,498] INFO: Model architecture:
[2025-12-13 12:07:05,498] INFO: MultiLayerLSTM(
  (emb): Embedding(234, 64, padding_idx=0)
  (lstm): LSTM(64, 8, batch_first=True)
  (dropout): Dropout(p=0, inplace=False)
  (fc): Linear(in_features=8, out_features=5, bias=True)
)
[2025-12-13 12:07:05,499] INFO: Total parameters      : 17,389
[2025-12-13 12:07:05,501] INFO: Trainable parameters  : 17,389
[2025-12-13 12:07:05,502] INFO: Frozen parameters     : 0
[2025-12-13 12:07:05,502] INFO: Embedding parameters  : 14,976
[2025-12-13 12:07:05,503] INFO: ==== MODEL SUMMARY END ====
[2025-12-13 12:07:07,463] INFO: Epoch 0001 | Loss: 1.581801 | Acc: 34.38%
[2025-12-13 12:07:07,470] INFO: Epoch 0002 | Loss: 1.463243 | Acc: 62.50%
[2025-12-13 12:07:07,477] INFO: Epoch 0003 | Loss: 1.307158 | Acc: 65.62%
[2025-12-13 12:07:07,484] INFO: Epoch 0004 | Loss: 1.130346 | Acc: 65.62%
[2025-12-13 12:07:07,492] INFO: Epoch 0005 | Loss: 0.946814 | Acc: 71.88%
[2025-12-13 12:07:07,498] INFO: Epoch 0006 | Loss: 0.769303 | Acc: 81.25%
[2025-12-13 12:07:07,506] INFO: Epoch 0007 | Loss: 0.610745 | Acc: 87.50%
[2025-12-13 12:07:07,512] INFO: Epoch 0008 | Loss: 0.480069 | Acc: 87.50%
[2025-12-13 12:07:07,519] INFO: Epoch 0009 | Loss: 0.382981 | Acc: 87.50%
[2025-12-13 12:07:07,524] INFO: Epoch 0010 | Loss: 0.305940 | Acc: 87.50%
[2025-12-13 12:07:07,531] INFO: Epoch 0011 | Loss: 0.236101 | Acc: 100.00%
[2025-12-13 12:07:07,540] INFO: Epoch 0012 | Loss: 0.178728 | Acc: 100.00%
[2025-12-13 12:07:07,545] INFO: Epoch 0013 | Loss: 0.137479 | Acc: 100.00%
[2025-12-13 12:07:07,553] INFO: Epoch 0014 | Loss: 0.107924 | Acc: 100.00%
[2025-12-13 12:07:07,560] INFO: Epoch 0015 | Loss: 0.084633 | Acc: 100.00%
[2025-12-13 12:07:07,568] INFO: Epoch 0016 | Loss: 0.065872 | Acc: 100.00%
[2025-12-13 12:07:07,576] INFO: Epoch 0017 | Loss: 0.051284 | Acc: 100.00%
[2025-12-13 12:07:07,582] INFO: Epoch 0018 | Loss: 0.040306 | Acc: 100.00%
[2025-12-13 12:07:07,588] INFO: Epoch 0019 | Loss: 0.032059 | Acc: 100.00%
[2025-12-13 12:07:07,595] INFO: Epoch 0020 | Loss: 0.025795 | Acc: 100.00%
[2025-12-13 12:07:07,604] INFO: Epoch 0021 | Loss: 0.020998 | Acc: 100.00%
[2025-12-13 12:07:07,610] INFO: Epoch 0022 | Loss: 0.017313 | Acc: 100.00%
[2025-12-13 12:07:07,617] INFO: Epoch 0023 | Loss: 0.014470 | Acc: 100.00%
[2025-12-13 12:07:07,625] INFO: Epoch 0024 | Loss: 0.012259 | Acc: 100.00%
[2025-12-13 12:07:07,632] INFO: Epoch 0025 | Loss: 0.010520 | Acc: 100.00%
[2025-12-13 12:07:07,639] INFO: Epoch 0026 | Loss: 0.009133 | Acc: 100.00%
[2025-12-13 12:07:07,645] INFO: Epoch 0027 | Loss: 0.008012 | Acc: 100.00%
[2025-12-13 12:07:07,651] INFO: Epoch 0028 | Loss: 0.007097 | Acc: 100.00%
[2025-12-13 12:07:07,658] INFO: Epoch 0029 | Loss: 0.006341 | Acc: 100.00%
[2025-12-13 12:07:07,664] INFO: Epoch 0030 | Loss: 0.005711 | Acc: 100.00%
[2025-12-13 12:07:07,672] INFO: Epoch 0031 | Loss: 0.005181 | Acc: 100.00%
[2025-12-13 12:07:07,680] INFO: Epoch 0032 | Loss: 0.004732 | Acc: 100.00%
[2025-12-13 12:07:07,688] INFO: Epoch 0033 | Loss: 0.004350 | Acc: 100.00%
[2025-12-13 12:07:07,694] INFO: Epoch 0034 | Loss: 0.004021 | Acc: 100.00%
[2025-12-13 12:07:07,700] INFO: Epoch 0035 | Loss: 0.003737 | Acc: 100.00%
[2025-12-13 12:07:07,707] INFO: Epoch 0036 | Loss: 0.003491 | Acc: 100.00%
[2025-12-13 12:07:07,715] INFO: Epoch 0037 | Loss: 0.003276 | Acc: 100.00%
[2025-12-13 12:07:07,722] INFO: Epoch 0038 | Loss: 0.003087 | Acc: 100.00%
[2025-12-13 12:07:07,727] INFO: Epoch 0039 | Loss: 0.002920 | Acc: 100.00%
[2025-12-13 12:07:07,734] INFO: Epoch 0040 | Loss: 0.002773 | Acc: 100.00%
[2025-12-13 12:07:07,739] INFO: Epoch 0041 | Loss: 0.002642 | Acc: 100.00%
[2025-12-13 12:07:07,745] INFO: Epoch 0042 | Loss: 0.002524 | Acc: 100.00%
[2025-12-13 12:07:07,751] INFO: Epoch 0043 | Loss: 0.002419 | Acc: 100.00%
[2025-12-13 12:07:07,758] INFO: Epoch 0044 | Loss: 0.002325 | Acc: 100.00%
[2025-12-13 12:07:07,764] INFO: Epoch 0045 | Loss: 0.002239 | Acc: 100.00%
[2025-12-13 12:07:07,772] INFO: Epoch 0046 | Loss: 0.002162 | Acc: 100.00%
[2025-12-13 12:07:07,779] INFO: Epoch 0047 | Loss: 0.002092 | Acc: 100.00%
[2025-12-13 12:07:07,785] INFO: Epoch 0048 | Loss: 0.002028 | Acc: 100.00%
[2025-12-13 12:07:07,792] INFO: Epoch 0049 | Loss: 0.001969 | Acc: 100.00%
[2025-12-13 12:07:07,801] INFO: Epoch 0050 | Loss: 0.001915 | Acc: 100.00%
[2025-12-13 12:07:07,806] INFO: Epoch 0051 | Loss: 0.001865 | Acc: 100.00%
[2025-12-13 12:07:07,812] INFO: Epoch 0052 | Loss: 0.001820 | Acc: 100.00%
[2025-12-13 12:07:07,821] INFO: Epoch 0053 | Loss: 0.001777 | Acc: 100.00%
[2025-12-13 12:07:07,828] INFO: Epoch 0054 | Loss: 0.001738 | Acc: 100.00%
[2025-12-13 12:07:07,834] INFO: Epoch 0055 | Loss: 0.001702 | Acc: 100.00%
[2025-12-13 12:07:07,842] INFO: Epoch 0056 | Loss: 0.001668 | Acc: 100.00%
[2025-12-13 12:07:07,848] INFO: Epoch 0057 | Loss: 0.001636 | Acc: 100.00%
[2025-12-13 12:07:07,856] INFO: Epoch 0058 | Loss: 0.001606 | Acc: 100.00%
[2025-12-13 12:07:07,862] INFO: Epoch 0059 | Loss: 0.001578 | Acc: 100.00%
[2025-12-13 12:07:07,868] INFO: Epoch 0060 | Loss: 0.001552 | Acc: 100.00%
[2025-12-13 12:07:07,876] INFO: Epoch 0061 | Loss: 0.001527 | Acc: 100.00%
[2025-12-13 12:07:07,882] INFO: Epoch 0062 | Loss: 0.001504 | Acc: 100.00%
[2025-12-13 12:07:07,888] INFO: Epoch 0063 | Loss: 0.001481 | Acc: 100.00%
[2025-12-13 12:07:07,894] INFO: Epoch 0064 | Loss: 0.001460 | Acc: 100.00%
[2025-12-13 12:07:07,900] INFO: Epoch 0065 | Loss: 0.001440 | Acc: 100.00%
[2025-12-13 12:07:07,906] INFO: Epoch 0066 | Loss: 0.001421 | Acc: 100.00%
[2025-12-13 12:07:07,912] INFO: Epoch 0067 | Loss: 0.001403 | Acc: 100.00%
[2025-12-13 12:07:07,918] INFO: Epoch 0068 | Loss: 0.001386 | Acc: 100.00%
[2025-12-13 12:07:07,923] INFO: Epoch 0069 | Loss: 0.001369 | Acc: 100.00%
[2025-12-13 12:07:07,928] INFO: Epoch 0070 | Loss: 0.001353 | Acc: 100.00%
[2025-12-13 12:07:07,935] INFO: Epoch 0071 | Loss: 0.001338 | Acc: 100.00%
[2025-12-13 12:07:07,942] INFO: Epoch 0072 | Loss: 0.001323 | Acc: 100.00%
[2025-12-13 12:07:07,950] INFO: Epoch 0073 | Loss: 0.001309 | Acc: 100.00%
[2025-12-13 12:07:07,956] INFO: Epoch 0074 | Loss: 0.001295 | Acc: 100.00%
[2025-12-13 12:07:07,962] INFO: Epoch 0075 | Loss: 0.001281 | Acc: 100.00%
[2025-12-13 12:07:07,969] INFO: Epoch 0076 | Loss: 0.001268 | Acc: 100.00%
[2025-12-13 12:07:07,975] INFO: Epoch 0077 | Loss: 0.001256 | Acc: 100.00%
[2025-12-13 12:07:07,982] INFO: Epoch 0078 | Loss: 0.001243 | Acc: 100.00%
[2025-12-13 12:07:07,987] INFO: Epoch 0079 | Loss: 0.001231 | Acc: 100.00%
[2025-12-13 12:07:07,993] INFO: Epoch 0080 | Loss: 0.001220 | Acc: 100.00%
[2025-12-13 12:07:07,999] INFO: Epoch 0081 | Loss: 0.001208 | Acc: 100.00%
[2025-12-13 12:07:08,005] INFO: Epoch 0082 | Loss: 0.001197 | Acc: 100.00%
[2025-12-13 12:07:08,010] INFO: Epoch 0083 | Loss: 0.001186 | Acc: 100.00%
[2025-12-13 12:07:08,016] INFO: Epoch 0084 | Loss: 0.001175 | Acc: 100.00%
[2025-12-13 12:07:08,022] INFO: Epoch 0085 | Loss: 0.001165 | Acc: 100.00%
[2025-12-13 12:07:08,028] INFO: Epoch 0086 | Loss: 0.001155 | Acc: 100.00%
[2025-12-13 12:07:08,035] INFO: Epoch 0087 | Loss: 0.001145 | Acc: 100.00%
[2025-12-13 12:07:08,041] INFO: Epoch 0088 | Loss: 0.001135 | Acc: 100.00%
[2025-12-13 12:07:08,047] INFO: Epoch 0089 | Loss: 0.001125 | Acc: 100.00%
[2025-12-13 12:07:08,053] INFO: Epoch 0090 | Loss: 0.001115 | Acc: 100.00%
[2025-12-13 12:07:08,059] INFO: Epoch 0091 | Loss: 0.001106 | Acc: 100.00%
[2025-12-13 12:07:08,068] INFO: Epoch 0092 | Loss: 0.001097 | Acc: 100.00%
[2025-12-13 12:07:08,077] INFO: Epoch 0093 | Loss: 0.001088 | Acc: 100.00%
[2025-12-13 12:07:08,084] INFO: Epoch 0094 | Loss: 0.001079 | Acc: 100.00%
[2025-12-13 12:07:08,090] INFO: Epoch 0095 | Loss: 0.001070 | Acc: 100.00%
[2025-12-13 12:07:08,097] INFO: Epoch 0096 | Loss: 0.001061 | Acc: 100.00%
[2025-12-13 12:07:08,104] INFO: Epoch 0097 | Loss: 0.001053 | Acc: 100.00%
[2025-12-13 12:07:08,112] INFO: Epoch 0098 | Loss: 0.001044 | Acc: 100.00%
[2025-12-13 12:07:08,119] INFO: Epoch 0099 | Loss: 0.001036 | Acc: 100.00%
[2025-12-13 12:07:08,125] INFO: Epoch 0100 | Loss: 0.001028 | Acc: 100.00%
[2025-12-13 12:07:08,134] INFO: Epoch 0101 | Loss: 0.001020 | Acc: 100.00%
[2025-12-13 12:07:08,141] INFO: Epoch 0102 | Loss: 0.001011 | Acc: 100.00%
[2025-12-13 12:07:08,148] INFO: Epoch 0103 | Loss: 0.001003 | Acc: 100.00%
[2025-12-13 12:07:08,155] INFO: Epoch 0104 | Loss: 0.000996 | Acc: 100.00%
[2025-12-13 12:07:08,156] INFO: ==== OVERFIT ACHIEVED ====
Train loss 0.000996 < 0.001
Stopping training.
[2025-12-13 12:07:08,157] INFO: Epoch 104 | Loss: 0.0010 | Acc: 100.00%
[2025-12-13 12:07:08,157] INFO: ==== SINGLE BATCH OVERFIT END ====
=== Finished at Sat Dec 13 12:07:09 UTC 2025 ===
