=== Starting pipeline at Sat Dec 13 16:38:06 UTC 2025 ===
Running script: 00_download_data.py
Downloading to /app/data/downloaded.zip ...
File downloaded successfully!
Running script: 01_data_preprocessing.py
ZIP extracted to: /app/extracted_zip
ERROR loading file.json: 'rating'
ERROR loading meta.json: 'str' object has no attribute 'get'
Base DF columns: Index(['text', 'rating'], dtype='object')
Consensus DF columns: Index(['text', 'rating'], dtype='object')
Base DF shape: (3737, 2)
Consensus DF shape: (2648, 2)
Number of rows filtered out from base_df because they exist in consensus: 615
Number of aggregated consensus texts (after averaging): 609
Consensus inference saved to: /app/data/inference.csv
Train/Val/Test split saved.
Running script: 02_train.py
[2025-12-13 16:38:34,187] INFO: ==== FULL DATASET TRAIN/VAL START ====
[2025-12-13 16:38:47,143] INFO: Using device: cuda
[2025-12-13 16:38:47,213] INFO: Train size: 2434, Val size: 272
[2025-12-13 16:38:47,255] INFO: collecting all words and their counts
[2025-12-13 16:38:47,255] INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
[2025-12-13 16:38:47,296] INFO: collected 24220 word types from a corpus of 120832 raw words and 2434 sentences
[2025-12-13 16:38:47,296] INFO: Creating a fresh vocabulary
[2025-12-13 16:38:47,468] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 24220 unique words (100.00% of original 24220, drops 0)', 'datetime': '2025-12-13T16:38:47.456031', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 16:38:47,468] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 120832 word corpus (100.00% of original 120832, drops 0)', 'datetime': '2025-12-13T16:38:47.468614', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 16:38:47,729] INFO: deleting the raw counts dictionary of 24220 items
[2025-12-13 16:38:47,731] INFO: sample=0.001 downsamples 19 most-common words
[2025-12-13 16:38:47,731] INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 101584.10254042316 word corpus (84.1%% of prior 120832)', 'datetime': '2025-12-13T16:38:47.731290', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 16:38:48,128] INFO: estimated required memory for 24220 words and 64 dimensions: 24510640 bytes
[2025-12-13 16:38:48,128] INFO: resetting layer weights
[2025-12-13 16:38:48,150] INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-13T16:38:48.150767', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}
[2025-12-13 16:38:48,151] INFO: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 24220 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-13T16:38:48.151140', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 16:38:48,293] INFO: EPOCH 0: training on 120832 raw words (101468 effective words) took 0.1s, 776825 effective words/s
[2025-12-13 16:38:48,420] INFO: EPOCH 1: training on 120832 raw words (101498 effective words) took 0.1s, 841687 effective words/s
[2025-12-13 16:38:48,542] INFO: EPOCH 2: training on 120832 raw words (101636 effective words) took 0.1s, 894521 effective words/s
[2025-12-13 16:38:48,663] INFO: EPOCH 3: training on 120832 raw words (101607 effective words) took 0.1s, 891091 effective words/s
[2025-12-13 16:38:48,781] INFO: EPOCH 4: training on 120832 raw words (101588 effective words) took 0.1s, 910277 effective words/s
[2025-12-13 16:38:48,781] INFO: Word2Vec lifecycle event {'msg': 'training on 604160 raw words (507797 effective words) took 0.6s, 805230 effective words/s', 'datetime': '2025-12-13T16:38:48.781891', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 16:38:48,782] INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=24220, vector_size=64, alpha=0.025>', 'datetime': '2025-12-13T16:38:48.782073', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}
[2025-12-13 16:38:48,782] INFO: Word2Vec trained. Vocab size: 24220 Embedding dim: 64
[2025-12-13 16:38:49,089] INFO: ==== CONFIG & HYPERPARAMETERS START ====
[2025-12-13 16:38:49,089] INFO: BATCH_SIZE          : 16
[2025-12-13 16:38:49,089] INFO: DATA_DIR            : /app/data
[2025-12-13 16:38:49,089] INFO: EARLY_STOP_PATIENCE : 40
[2025-12-13 16:38:49,089] INFO: FC_DROPOUT          : 0.4
[2025-12-13 16:38:49,089] INFO: LEARNING_RATE       : 0.001
[2025-12-13 16:38:49,090] INFO: LOG_FILE            : /app/logs/run.log
[2025-12-13 16:38:49,090] INFO: LSTM_DROPOUT        : 0.3
[2025-12-13 16:38:49,090] INFO: LSTM_HIDDEN_DIM     : 128
[2025-12-13 16:38:49,090] INFO: MODEL_NAME          : LSTM
[2025-12-13 16:38:49,090] INFO: MODEL_PATH          : /app/data/lstm_model.pth
[2025-12-13 16:38:49,090] INFO: NUM_CLASSES         : 5
[2025-12-13 16:38:49,090] INFO: NUM_EPOCHS          : 1000
[2025-12-13 16:38:49,090] INFO: NUM_LSTM_LAYERS     : 2
[2025-12-13 16:38:49,091] INFO: SEQ_LEN             : 200
[2025-12-13 16:38:49,091] INFO: W2V_PATH            : /app/data/w2v.model
[2025-12-13 16:38:49,091] INFO: WEIGHT_DECAY        : 0.0001
[2025-12-13 16:38:49,091] INFO: WORD2_VEC_MIN_COUNT : 1
[2025-12-13 16:38:49,091] INFO: WORD2_VEC_VECTOR_SIZE: 64
[2025-12-13 16:38:49,091] INFO: ==== CONFIG & HYPERPARAMETERS END ====
[2025-12-13 16:39:03,319] INFO: ==== MODEL SUMMARY START ====
[2025-12-13 16:39:03,320] INFO: Model architecture:
[2025-12-13 16:39:03,320] INFO: MultiLayerLSTM(
  (emb): Embedding(24221, 64, padding_idx=0)
  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)
  (dropout): Dropout(p=0.4, inplace=False)
  (fc): Linear(in_features=128, out_features=5, bias=True)
)
[2025-12-13 16:39:03,320] INFO: Total parameters      : 1,782,213
[2025-12-13 16:39:03,320] INFO: Trainable parameters  : 1,782,213
[2025-12-13 16:39:03,320] INFO: Frozen parameters     : 0
[2025-12-13 16:39:03,320] INFO: Embedding parameters  : 1,550,144
[2025-12-13 16:39:03,320] INFO: ==== MODEL SUMMARY END ====
[2025-12-13 16:39:11,924] INFO: Epoch 1/1000 | Train Loss: 0.8759 | Train Acc: 30.98% | Val Loss: 0.8516 | Val Acc: 32.35%
[2025-12-13 16:39:11,978] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:14,482] INFO: Epoch 2/1000 | Train Loss: 0.8706 | Train Acc: 31.80% | Val Loss: 0.8462 | Val Acc: 32.35%
[2025-12-13 16:39:14,509] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:17,021] INFO: Epoch 3/1000 | Train Loss: 0.8550 | Train Acc: 33.11% | Val Loss: 0.8469 | Val Acc: 32.35%
[2025-12-13 16:39:17,022] INFO: No improvement in val loss for 1 epoch(s).
[2025-12-13 16:39:19,458] INFO: Epoch 4/1000 | Train Loss: 0.8587 | Train Acc: 31.84% | Val Loss: 0.8515 | Val Acc: 32.72%
[2025-12-13 16:39:19,458] INFO: No improvement in val loss for 2 epoch(s).
[2025-12-13 16:39:22,045] INFO: Epoch 5/1000 | Train Loss: 0.8843 | Train Acc: 32.13% | Val Loss: 0.8609 | Val Acc: 32.35%
[2025-12-13 16:39:22,045] INFO: No improvement in val loss for 3 epoch(s).
[2025-12-13 16:39:24,731] INFO: Epoch 6/1000 | Train Loss: 0.8671 | Train Acc: 32.21% | Val Loss: 0.8431 | Val Acc: 34.93%
[2025-12-13 16:39:24,758] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:27,296] INFO: Epoch 7/1000 | Train Loss: 0.8669 | Train Acc: 32.25% | Val Loss: 0.8430 | Val Acc: 34.56%
[2025-12-13 16:39:27,321] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:29,775] INFO: Epoch 8/1000 | Train Loss: 0.8544 | Train Acc: 33.11% | Val Loss: 0.8082 | Val Acc: 36.76%
[2025-12-13 16:39:29,800] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:32,240] INFO: Epoch 9/1000 | Train Loss: 0.8171 | Train Acc: 34.43% | Val Loss: 0.8255 | Val Acc: 34.93%
[2025-12-13 16:39:32,240] INFO: No improvement in val loss for 4 epoch(s).
[2025-12-13 16:39:34,677] INFO: Epoch 10/1000 | Train Loss: 0.7818 | Train Acc: 38.25% | Val Loss: 0.8381 | Val Acc: 29.41%
[2025-12-13 16:39:34,677] INFO: No improvement in val loss for 5 epoch(s).
[2025-12-13 16:39:37,119] INFO: Epoch 11/1000 | Train Loss: 0.7738 | Train Acc: 39.85% | Val Loss: 0.7960 | Val Acc: 36.76%
[2025-12-13 16:39:37,145] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:39,635] INFO: Epoch 12/1000 | Train Loss: 0.7610 | Train Acc: 41.45% | Val Loss: 0.7872 | Val Acc: 38.24%
[2025-12-13 16:39:39,659] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 16:39:42,264] INFO: Epoch 13/1000 | Train Loss: 0.7491 | Train Acc: 42.40% | Val Loss: 0.8099 | Val Acc: 36.03%
[2025-12-13 16:39:42,265] INFO: No improvement in val loss for 6 epoch(s).
[2025-12-13 16:39:44,746] INFO: Epoch 14/1000 | Train Loss: 0.7187 | Train Acc: 44.33% | Val Loss: 0.7914 | Val Acc: 37.87%
[2025-12-13 16:39:44,746] INFO: No improvement in val loss for 7 epoch(s).
[2025-12-13 16:39:47,351] INFO: Epoch 15/1000 | Train Loss: 0.6781 | Train Acc: 45.97% | Val Loss: 0.8139 | Val Acc: 37.13%
[2025-12-13 16:39:47,352] INFO: No improvement in val loss for 8 epoch(s).
[2025-12-13 16:39:49,838] INFO: Epoch 16/1000 | Train Loss: 0.6355 | Train Acc: 48.48% | Val Loss: 0.8844 | Val Acc: 38.24%
[2025-12-13 16:39:49,838] INFO: No improvement in val loss for 9 epoch(s).
[2025-12-13 16:39:52,280] INFO: Epoch 17/1000 | Train Loss: 0.5781 | Train Acc: 52.71% | Val Loss: 0.8862 | Val Acc: 36.03%
[2025-12-13 16:39:52,280] INFO: No improvement in val loss for 10 epoch(s).
[2025-12-13 16:39:54,757] INFO: Epoch 18/1000 | Train Loss: 0.4773 | Train Acc: 60.93% | Val Loss: 1.0297 | Val Acc: 38.97%
[2025-12-13 16:39:54,757] INFO: No improvement in val loss for 11 epoch(s).
[2025-12-13 16:39:57,232] INFO: Epoch 19/1000 | Train Loss: 0.4160 | Train Acc: 64.26% | Val Loss: 1.0540 | Val Acc: 39.34%
[2025-12-13 16:39:57,232] INFO: No improvement in val loss for 12 epoch(s).
[2025-12-13 16:39:59,677] INFO: Epoch 20/1000 | Train Loss: 0.3646 | Train Acc: 68.36% | Val Loss: 1.1127 | Val Acc: 38.24%
[2025-12-13 16:39:59,677] INFO: No improvement in val loss for 13 epoch(s).
[2025-12-13 16:40:02,118] INFO: Epoch 21/1000 | Train Loss: 0.3290 | Train Acc: 70.75% | Val Loss: 1.3210 | Val Acc: 36.03%
[2025-12-13 16:40:02,118] INFO: No improvement in val loss for 14 epoch(s).
[2025-12-13 16:40:04,556] INFO: Epoch 22/1000 | Train Loss: 0.3008 | Train Acc: 71.49% | Val Loss: 1.2671 | Val Acc: 42.65%
[2025-12-13 16:40:04,556] INFO: No improvement in val loss for 15 epoch(s).
[2025-12-13 16:40:07,050] INFO: Epoch 23/1000 | Train Loss: 0.2891 | Train Acc: 73.50% | Val Loss: 1.2833 | Val Acc: 37.50%
[2025-12-13 16:40:07,050] INFO: No improvement in val loss for 16 epoch(s).
[2025-12-13 16:40:09,551] INFO: Epoch 24/1000 | Train Loss: 0.2532 | Train Acc: 76.87% | Val Loss: 1.5548 | Val Acc: 36.76%
[2025-12-13 16:40:09,551] INFO: No improvement in val loss for 17 epoch(s).
[2025-12-13 16:40:12,002] INFO: Epoch 25/1000 | Train Loss: 0.2348 | Train Acc: 78.84% | Val Loss: 1.4935 | Val Acc: 38.60%
[2025-12-13 16:40:12,002] INFO: No improvement in val loss for 18 epoch(s).
[2025-12-13 16:40:14,455] INFO: Epoch 26/1000 | Train Loss: 0.2101 | Train Acc: 81.43% | Val Loss: 1.5854 | Val Acc: 39.71%
[2025-12-13 16:40:14,455] INFO: No improvement in val loss for 19 epoch(s).
[2025-12-13 16:40:16,897] INFO: Epoch 27/1000 | Train Loss: 0.1958 | Train Acc: 82.70% | Val Loss: 1.6157 | Val Acc: 37.87%
[2025-12-13 16:40:16,897] INFO: No improvement in val loss for 20 epoch(s).
[2025-12-13 16:40:19,342] INFO: Epoch 28/1000 | Train Loss: 0.1801 | Train Acc: 84.68% | Val Loss: 1.7164 | Val Acc: 39.34%
[2025-12-13 16:40:19,343] INFO: No improvement in val loss for 21 epoch(s).
[2025-12-13 16:40:21,784] INFO: Epoch 29/1000 | Train Loss: 0.1851 | Train Acc: 83.65% | Val Loss: 1.6786 | Val Acc: 37.87%
[2025-12-13 16:40:21,785] INFO: No improvement in val loss for 22 epoch(s).
[2025-12-13 16:40:24,254] INFO: Epoch 30/1000 | Train Loss: 0.1870 | Train Acc: 82.05% | Val Loss: 1.7162 | Val Acc: 38.24%
[2025-12-13 16:40:24,254] INFO: No improvement in val loss for 23 epoch(s).
[2025-12-13 16:40:26,701] INFO: Epoch 31/1000 | Train Loss: 0.1804 | Train Acc: 84.06% | Val Loss: 1.7292 | Val Acc: 39.71%
[2025-12-13 16:40:26,701] INFO: No improvement in val loss for 24 epoch(s).
[2025-12-13 16:40:29,146] INFO: Epoch 32/1000 | Train Loss: 0.1706 | Train Acc: 86.15% | Val Loss: 1.7177 | Val Acc: 38.60%
[2025-12-13 16:40:29,146] INFO: No improvement in val loss for 25 epoch(s).
[2025-12-13 16:40:31,595] INFO: Epoch 33/1000 | Train Loss: 0.1622 | Train Acc: 86.69% | Val Loss: 1.8072 | Val Acc: 39.71%
[2025-12-13 16:40:31,595] INFO: No improvement in val loss for 26 epoch(s).
[2025-12-13 16:40:34,042] INFO: Epoch 34/1000 | Train Loss: 0.1616 | Train Acc: 85.70% | Val Loss: 1.8251 | Val Acc: 39.71%
[2025-12-13 16:40:34,042] INFO: No improvement in val loss for 27 epoch(s).
[2025-12-13 16:40:36,501] INFO: Epoch 35/1000 | Train Loss: 0.1579 | Train Acc: 85.95% | Val Loss: 1.7929 | Val Acc: 40.44%
[2025-12-13 16:40:36,501] INFO: No improvement in val loss for 28 epoch(s).
[2025-12-13 16:40:38,982] INFO: Epoch 36/1000 | Train Loss: 0.1583 | Train Acc: 86.52% | Val Loss: 1.7511 | Val Acc: 39.34%
[2025-12-13 16:40:38,983] INFO: No improvement in val loss for 29 epoch(s).
[2025-12-13 16:40:41,444] INFO: Epoch 37/1000 | Train Loss: 0.1436 | Train Acc: 87.84% | Val Loss: 1.7499 | Val Acc: 38.60%
[2025-12-13 16:40:41,444] INFO: No improvement in val loss for 30 epoch(s).
[2025-12-13 16:40:43,913] INFO: Epoch 38/1000 | Train Loss: 0.1463 | Train Acc: 87.55% | Val Loss: 1.8373 | Val Acc: 36.40%
[2025-12-13 16:40:43,913] INFO: No improvement in val loss for 31 epoch(s).
[2025-12-13 16:40:46,407] INFO: Epoch 39/1000 | Train Loss: 0.1486 | Train Acc: 87.22% | Val Loss: 1.7117 | Val Acc: 37.87%
[2025-12-13 16:40:46,407] INFO: No improvement in val loss for 32 epoch(s).
[2025-12-13 16:40:48,879] INFO: Epoch 40/1000 | Train Loss: 0.1494 | Train Acc: 86.77% | Val Loss: 1.8033 | Val Acc: 39.71%
[2025-12-13 16:40:48,880] INFO: No improvement in val loss for 33 epoch(s).
[2025-12-13 16:40:51,399] INFO: Epoch 41/1000 | Train Loss: 0.1372 | Train Acc: 88.82% | Val Loss: 1.8375 | Val Acc: 39.34%
[2025-12-13 16:40:51,399] INFO: No improvement in val loss for 34 epoch(s).
[2025-12-13 16:40:53,939] INFO: Epoch 42/1000 | Train Loss: 0.1324 | Train Acc: 88.82% | Val Loss: 1.9678 | Val Acc: 36.76%
[2025-12-13 16:40:53,940] INFO: No improvement in val loss for 35 epoch(s).
[2025-12-13 16:40:56,445] INFO: Epoch 43/1000 | Train Loss: 0.1209 | Train Acc: 90.47% | Val Loss: 1.8800 | Val Acc: 36.76%
[2025-12-13 16:40:56,446] INFO: No improvement in val loss for 36 epoch(s).
[2025-12-13 16:40:58,935] INFO: Epoch 44/1000 | Train Loss: 0.1180 | Train Acc: 91.04% | Val Loss: 1.9608 | Val Acc: 35.29%
[2025-12-13 16:40:58,936] INFO: No improvement in val loss for 37 epoch(s).
[2025-12-13 16:41:01,395] INFO: Epoch 45/1000 | Train Loss: 0.1316 | Train Acc: 88.41% | Val Loss: 1.8672 | Val Acc: 38.60%
[2025-12-13 16:41:01,395] INFO: No improvement in val loss for 38 epoch(s).
[2025-12-13 16:41:03,849] INFO: Epoch 46/1000 | Train Loss: 0.1399 | Train Acc: 87.67% | Val Loss: 1.8483 | Val Acc: 36.40%
[2025-12-13 16:41:03,849] INFO: No improvement in val loss for 39 epoch(s).
[2025-12-13 16:41:06,310] INFO: Epoch 47/1000 | Train Loss: 0.1470 | Train Acc: 87.51% | Val Loss: 1.7415 | Val Acc: 38.60%
[2025-12-13 16:41:06,310] INFO: No improvement in val loss for 40 epoch(s).
[2025-12-13 16:41:06,310] INFO: Early stopping triggered after 40 epochs with no improvement.
[2025-12-13 16:41:06,311] INFO: Word2Vec lifecycle event {'fname_or_handle': '/app/data/w2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-12-13T16:41:06.310991', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'saving'}
[2025-12-13 16:41:06,311] INFO: not storing attribute cum_table
[2025-12-13 16:41:06,369] INFO: saved /app/data/w2v.model
[2025-12-13 16:41:06,370] INFO: Word2Vec model saved to /app/data/w2v.model
[2025-12-13 16:41:06,370] INFO: ==== FULL DATASET TRAIN/VAL END ====
Running script: 03_evaluation.py
[2025-12-13 16:41:13,336] INFO: ==== EVALUATION START ====
[2025-12-13 16:41:15,487] INFO: Using device: cuda
[2025-12-13 16:41:15,537] INFO: GPU: NVIDIA GeForce GTX 1050 Ti, CUDA capability: 6.1
[2025-12-13 16:41:15,554] INFO: Loaded test dataset with 307 samples.
[2025-12-13 16:41:15,565] INFO: loading Word2Vec object from /app/data/w2v.model
[2025-12-13 16:41:15,624] INFO: loading wv recursively from /app/data/w2v.model.wv.* with mmap=None
[2025-12-13 16:41:15,626] INFO: setting ignored attribute cum_table to None
[2025-12-13 16:41:16,037] INFO: Word2Vec lifecycle event {'fname': '/app/data/w2v.model', 'datetime': '2025-12-13T16:41:16.033748', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-13 16:41:16,037] INFO: Word2Vec model loaded from /app/data/w2v.model with vocab size 24220 and vector size 64
[2025-12-13 16:41:22,074] INFO: LSTM model loaded from /app/data/lstm_model.pth and ready for evaluation.
[2025-12-13 16:41:22,075] INFO: Starting LSTM evaluation on test set...
[2025-12-13 16:41:22,636] INFO: LSTM Test Loss: 1.3465
[2025-12-13 16:41:22,636] INFO: LSTM Test Accuracy: 40.07%
[2025-12-13 16:41:22,637] INFO: LSTM F1-score (weighted): 0.3626
[2025-12-13 16:41:22,639] INFO: LSTM Confusion Matrix:
[[ 0  0  6  4  3]
 [ 0  0 14 13  8]
 [ 0  0 19 30 16]
 [ 0  0 19 52 35]
 [ 0  0  5 31 52]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 16:41:22,661] INFO: LSTM Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.00      0.00      0.00        35
           2       0.30      0.29      0.30        65
           3       0.40      0.49      0.44       106
           4       0.46      0.59      0.51        88

    accuracy                           0.40       307
   macro avg       0.23      0.27      0.25       307
weighted avg       0.33      0.40      0.36       307

[2025-12-13 16:41:22,661] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 16:41:22,664] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 16:41:22,704] INFO: Train dataset loaded: 2434 samples.
[2025-12-13 16:41:25,305] INFO: TF-IDF + Logistic Regression trained.
[2025-12-13 16:41:25,366] INFO: TF-IDF + Logistic Regression Test Accuracy: 36.81%
[2025-12-13 16:41:25,367] INFO: TF-IDF + Logistic Regression F1-score (weighted): 0.3350
[2025-12-13 16:41:25,372] INFO: TF-IDF + Logistic Regression Confusion Matrix:
[[ 0  2  4  2  5]
 [ 0  1 16 14  4]
 [ 0  4  9 33 19]
 [ 0  1 24 47 34]
 [ 0  0  8 24 56]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 16:41:25,392] INFO: TF-IDF Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.12      0.03      0.05        35
           2       0.15      0.14      0.14        65
           3       0.39      0.44      0.42       106
           4       0.47      0.64      0.54        88

    accuracy                           0.37       307
   macro avg       0.23      0.25      0.23       307
weighted avg       0.32      0.37      0.34       307

[2025-12-13 16:41:25,393] INFO: ==== EVALUATION END ====
=== Finished at Sat Dec 13 16:41:26 UTC 2025 ===
