=== Starting pipeline at Sat Dec 13 14:54:11 UTC 2025 ===
Running script: 00_download_data.py
Downloading to /app/data/downloaded.zip ...
File downloaded successfully!
Running script: 01_data_preprocessing.py
ZIP extracted to: /app/extracted_zip
ERROR loading file.json: 'rating'
ERROR loading meta.json: 'str' object has no attribute 'get'
Base DF columns: Index(['text', 'rating'], dtype='object')
Consensus DF columns: Index(['text', 'rating'], dtype='object')
Base DF shape: (3737, 2)
Consensus DF shape: (2648, 2)
Number of rows filtered out from base_df because they exist in consensus: 615
Number of aggregated consensus texts (after averaging): 609
Consensus inference saved to: /app/data/inference.csv
Train/Val/Test split saved.
Running script: 02_train.py
[2025-12-13 14:54:24,312] INFO: ==== FULL DATASET TRAIN/VAL START ====
[2025-12-13 14:54:26,899] INFO: Using device: cuda
[2025-12-13 14:54:26,954] INFO: Train size: 2434, Val size: 272
[2025-12-13 14:54:26,976] INFO: collecting all words and their counts
[2025-12-13 14:54:26,976] INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
[2025-12-13 14:54:26,987] INFO: collected 7072 word types from a corpus of 23582 raw words and 2434 sentences
[2025-12-13 14:54:26,987] INFO: Creating a fresh vocabulary
[2025-12-13 14:54:27,036] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 7072 unique words (100.00% of original 7072, drops 0)', 'datetime': '2025-12-13T14:54:27.034224', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 14:54:27,036] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 23582 word corpus (100.00% of original 23582, drops 0)', 'datetime': '2025-12-13T14:54:27.036895', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 14:54:27,120] INFO: deleting the raw counts dictionary of 7072 items
[2025-12-13 14:54:27,120] INFO: sample=0.001 downsamples 29 most-common words
[2025-12-13 14:54:27,120] INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 18930.92178751305 word corpus (80.3%% of prior 23582)', 'datetime': '2025-12-13T14:54:27.120871', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 14:54:27,238] INFO: estimated required memory for 7072 words and 64 dimensions: 7156864 bytes
[2025-12-13 14:54:27,238] INFO: resetting layer weights
[2025-12-13 14:54:27,247] INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-13T14:54:27.247877', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}
[2025-12-13 14:54:27,248] INFO: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 7072 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-13T14:54:27.248188', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 14:54:27,289] INFO: EPOCH 0: training on 23582 raw words (18923 effective words) took 0.0s, 572252 effective words/s
[2025-12-13 14:54:27,327] INFO: EPOCH 1: training on 23582 raw words (18939 effective words) took 0.0s, 620692 effective words/s
[2025-12-13 14:54:27,364] INFO: EPOCH 2: training on 23582 raw words (18977 effective words) took 0.0s, 700015 effective words/s
[2025-12-13 14:54:27,405] INFO: EPOCH 3: training on 23582 raw words (18946 effective words) took 0.0s, 577187 effective words/s
[2025-12-13 14:54:27,444] INFO: EPOCH 4: training on 23582 raw words (18945 effective words) took 0.0s, 609793 effective words/s
[2025-12-13 14:54:27,445] INFO: Word2Vec lifecycle event {'msg': 'training on 117910 raw words (94730 effective words) took 0.2s, 481405 effective words/s', 'datetime': '2025-12-13T14:54:27.445258', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 14:54:27,445] INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7072, vector_size=64, alpha=0.025>', 'datetime': '2025-12-13T14:54:27.445439', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}
[2025-12-13 14:54:27,445] INFO: Word2Vec trained. Vocab size: 7072 Embedding dim: 64
[2025-12-13 14:54:27,464] INFO: ==== CONFIG & HYPERPARAMETERS START ====
[2025-12-13 14:54:27,464] INFO: BATCH_SIZE          : 32
[2025-12-13 14:54:27,464] INFO: DATA_DIR            : /app/data
[2025-12-13 14:54:27,464] INFO: EARLY_STOP_PATIENCE : 40
[2025-12-13 14:54:27,464] INFO: FC_DROPOUT          : 0.1
[2025-12-13 14:54:27,465] INFO: LEARNING_RATE       : 0.04
[2025-12-13 14:54:27,465] INFO: LOG_FILE            : /app/logs/run.log
[2025-12-13 14:54:27,465] INFO: LSTM_DROPOUT        : 0.1
[2025-12-13 14:54:27,465] INFO: LSTM_HIDDEN_DIM     : 8
[2025-12-13 14:54:27,465] INFO: MODEL_NAME          : LSTM
[2025-12-13 14:54:27,465] INFO: MODEL_PATH          : /app/data/lstm_model.pth
[2025-12-13 14:54:27,465] INFO: NUM_CLASSES         : 5
[2025-12-13 14:54:27,465] INFO: NUM_EPOCHS          : 1000
[2025-12-13 14:54:27,466] INFO: NUM_LSTM_LAYERS     : 1
[2025-12-13 14:54:27,468] INFO: SEQ_LEN             : 10
[2025-12-13 14:54:27,468] INFO: W2V_PATH            : /app/data/w2v.model
[2025-12-13 14:54:27,469] INFO: WORD2_VEC_MIN_COUNT : 1
[2025-12-13 14:54:27,470] INFO: WORD2_VEC_VECTOR_SIZE: 64
[2025-12-13 14:54:27,471] INFO: ==== CONFIG & HYPERPARAMETERS END ====
[2025-12-13 14:54:33,926] INFO: ==== MODEL SUMMARY START ====
[2025-12-13 14:54:33,926] INFO: Model architecture:
[2025-12-13 14:54:33,926] INFO: MultiLayerLSTM(
  (emb): Embedding(7073, 64, padding_idx=0)
  (lstm): LSTM(64, 8, batch_first=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (fc): Linear(in_features=8, out_features=5, bias=True)
)
[2025-12-13 14:54:33,927] INFO: Total parameters      : 455,085
[2025-12-13 14:54:33,927] INFO: Trainable parameters  : 455,085
[2025-12-13 14:54:33,928] INFO: Frozen parameters     : 0
[2025-12-13 14:54:33,928] INFO: Embedding parameters  : 452,672
[2025-12-13 14:54:33,928] INFO: ==== MODEL SUMMARY END ====
[2025-12-13 14:54:36,386] INFO: Epoch 1/1000 | Train Loss: 1.4563 | Train Acc: 32.70% | Val Loss: 1.4397 | Val Acc: 30.51%
[2025-12-13 14:54:36,399] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 14:54:36,908] INFO: Epoch 2/1000 | Train Loss: 1.2209 | Train Acc: 52.30% | Val Loss: 1.7936 | Val Acc: 30.88%
[2025-12-13 14:54:36,908] INFO: No improvement in val loss for 1 epoch(s).
[2025-12-13 14:54:37,522] INFO: Epoch 3/1000 | Train Loss: 1.0214 | Train Acc: 61.63% | Val Loss: 1.9723 | Val Acc: 31.25%
[2025-12-13 14:54:37,523] INFO: No improvement in val loss for 2 epoch(s).
[2025-12-13 14:54:38,217] INFO: Epoch 4/1000 | Train Loss: 0.8847 | Train Acc: 66.60% | Val Loss: 2.4297 | Val Acc: 31.99%
[2025-12-13 14:54:38,217] INFO: No improvement in val loss for 3 epoch(s).
[2025-12-13 14:54:38,930] INFO: Epoch 5/1000 | Train Loss: 0.8005 | Train Acc: 70.87% | Val Loss: 2.3231 | Val Acc: 31.25%
[2025-12-13 14:54:38,931] INFO: No improvement in val loss for 4 epoch(s).
[2025-12-13 14:54:39,636] INFO: Epoch 6/1000 | Train Loss: 0.7123 | Train Acc: 75.14% | Val Loss: 2.3192 | Val Acc: 33.82%
[2025-12-13 14:54:39,636] INFO: No improvement in val loss for 5 epoch(s).
[2025-12-13 14:54:40,342] INFO: Epoch 7/1000 | Train Loss: 0.6645 | Train Acc: 76.29% | Val Loss: 2.3034 | Val Acc: 33.46%
[2025-12-13 14:54:40,342] INFO: No improvement in val loss for 6 epoch(s).
[2025-12-13 14:54:41,050] INFO: Epoch 8/1000 | Train Loss: 0.6027 | Train Acc: 79.09% | Val Loss: 2.6120 | Val Acc: 32.72%
[2025-12-13 14:54:41,050] INFO: No improvement in val loss for 7 epoch(s).
[2025-12-13 14:54:41,758] INFO: Epoch 9/1000 | Train Loss: 0.5741 | Train Acc: 79.42% | Val Loss: 2.5647 | Val Acc: 33.46%
[2025-12-13 14:54:41,759] INFO: No improvement in val loss for 8 epoch(s).
[2025-12-13 14:54:42,471] INFO: Epoch 10/1000 | Train Loss: 0.5528 | Train Acc: 80.44% | Val Loss: 2.7474 | Val Acc: 30.88%
[2025-12-13 14:54:42,471] INFO: No improvement in val loss for 9 epoch(s).
[2025-12-13 14:54:43,168] INFO: Epoch 11/1000 | Train Loss: 0.5404 | Train Acc: 81.06% | Val Loss: 2.4933 | Val Acc: 32.35%
[2025-12-13 14:54:43,168] INFO: No improvement in val loss for 10 epoch(s).
[2025-12-13 14:54:43,878] INFO: Epoch 12/1000 | Train Loss: 0.5148 | Train Acc: 81.47% | Val Loss: 2.5327 | Val Acc: 31.62%
[2025-12-13 14:54:43,878] INFO: No improvement in val loss for 11 epoch(s).
[2025-12-13 14:54:44,580] INFO: Epoch 13/1000 | Train Loss: 0.4811 | Train Acc: 82.83% | Val Loss: 2.7719 | Val Acc: 34.19%
[2025-12-13 14:54:44,580] INFO: No improvement in val loss for 12 epoch(s).
[2025-12-13 14:54:45,274] INFO: Epoch 14/1000 | Train Loss: 0.4694 | Train Acc: 82.54% | Val Loss: 2.6865 | Val Acc: 33.46%
[2025-12-13 14:54:45,274] INFO: No improvement in val loss for 13 epoch(s).
[2025-12-13 14:54:45,975] INFO: Epoch 15/1000 | Train Loss: 0.4984 | Train Acc: 82.87% | Val Loss: 2.6487 | Val Acc: 33.46%
[2025-12-13 14:54:45,975] INFO: No improvement in val loss for 14 epoch(s).
[2025-12-13 14:54:46,667] INFO: Epoch 16/1000 | Train Loss: 0.5056 | Train Acc: 81.27% | Val Loss: 2.5170 | Val Acc: 32.72%
[2025-12-13 14:54:46,667] INFO: No improvement in val loss for 15 epoch(s).
[2025-12-13 14:54:47,383] INFO: Epoch 17/1000 | Train Loss: 0.4636 | Train Acc: 83.53% | Val Loss: 2.6249 | Val Acc: 33.46%
[2025-12-13 14:54:47,383] INFO: No improvement in val loss for 16 epoch(s).
[2025-12-13 14:54:48,131] INFO: Epoch 18/1000 | Train Loss: 0.4458 | Train Acc: 84.59% | Val Loss: 2.6826 | Val Acc: 33.82%
[2025-12-13 14:54:48,131] INFO: No improvement in val loss for 17 epoch(s).
[2025-12-13 14:54:48,848] INFO: Epoch 19/1000 | Train Loss: 0.4372 | Train Acc: 84.84% | Val Loss: 2.9651 | Val Acc: 32.72%
[2025-12-13 14:54:48,848] INFO: No improvement in val loss for 18 epoch(s).
[2025-12-13 14:54:49,547] INFO: Epoch 20/1000 | Train Loss: 0.4354 | Train Acc: 84.47% | Val Loss: 3.0865 | Val Acc: 33.82%
[2025-12-13 14:54:49,547] INFO: No improvement in val loss for 19 epoch(s).
[2025-12-13 14:54:50,250] INFO: Epoch 21/1000 | Train Loss: 0.4194 | Train Acc: 85.74% | Val Loss: 3.0912 | Val Acc: 33.46%
[2025-12-13 14:54:50,251] INFO: No improvement in val loss for 20 epoch(s).
[2025-12-13 14:54:50,952] INFO: Epoch 22/1000 | Train Loss: 0.4182 | Train Acc: 85.70% | Val Loss: 3.1927 | Val Acc: 34.56%
[2025-12-13 14:54:50,952] INFO: No improvement in val loss for 21 epoch(s).
[2025-12-13 14:54:51,655] INFO: Epoch 23/1000 | Train Loss: 0.4028 | Train Acc: 86.48% | Val Loss: 3.2473 | Val Acc: 33.82%
[2025-12-13 14:54:51,655] INFO: No improvement in val loss for 22 epoch(s).
[2025-12-13 14:54:52,362] INFO: Epoch 24/1000 | Train Loss: 0.3929 | Train Acc: 86.77% | Val Loss: 3.1582 | Val Acc: 30.88%
[2025-12-13 14:54:52,362] INFO: No improvement in val loss for 23 epoch(s).
[2025-12-13 14:54:53,064] INFO: Epoch 25/1000 | Train Loss: 0.3988 | Train Acc: 86.36% | Val Loss: 3.1733 | Val Acc: 30.51%
[2025-12-13 14:54:53,064] INFO: No improvement in val loss for 24 epoch(s).
[2025-12-13 14:54:53,762] INFO: Epoch 26/1000 | Train Loss: 0.3889 | Train Acc: 86.65% | Val Loss: 3.1297 | Val Acc: 33.46%
[2025-12-13 14:54:53,762] INFO: No improvement in val loss for 25 epoch(s).
[2025-12-13 14:54:54,463] INFO: Epoch 27/1000 | Train Loss: 0.3818 | Train Acc: 86.89% | Val Loss: 3.2972 | Val Acc: 28.68%
[2025-12-13 14:54:54,463] INFO: No improvement in val loss for 26 epoch(s).
[2025-12-13 14:54:55,166] INFO: Epoch 28/1000 | Train Loss: 0.4224 | Train Acc: 85.62% | Val Loss: 3.1790 | Val Acc: 31.25%
[2025-12-13 14:54:55,166] INFO: No improvement in val loss for 27 epoch(s).
[2025-12-13 14:54:55,860] INFO: Epoch 29/1000 | Train Loss: 0.3945 | Train Acc: 86.28% | Val Loss: 3.2780 | Val Acc: 33.09%
[2025-12-13 14:54:55,860] INFO: No improvement in val loss for 28 epoch(s).
[2025-12-13 14:54:56,556] INFO: Epoch 30/1000 | Train Loss: 0.3996 | Train Acc: 85.87% | Val Loss: 3.2074 | Val Acc: 31.99%
[2025-12-13 14:54:56,556] INFO: No improvement in val loss for 29 epoch(s).
[2025-12-13 14:54:57,253] INFO: Epoch 31/1000 | Train Loss: 0.3988 | Train Acc: 85.91% | Val Loss: 3.2028 | Val Acc: 31.62%
[2025-12-13 14:54:57,253] INFO: No improvement in val loss for 30 epoch(s).
[2025-12-13 14:54:57,948] INFO: Epoch 32/1000 | Train Loss: 0.3855 | Train Acc: 86.57% | Val Loss: 3.2994 | Val Acc: 30.51%
[2025-12-13 14:54:57,948] INFO: No improvement in val loss for 31 epoch(s).
[2025-12-13 14:54:58,641] INFO: Epoch 33/1000 | Train Loss: 0.3830 | Train Acc: 86.94% | Val Loss: 3.3392 | Val Acc: 30.15%
[2025-12-13 14:54:58,641] INFO: No improvement in val loss for 32 epoch(s).
[2025-12-13 14:54:59,328] INFO: Epoch 34/1000 | Train Loss: 0.4031 | Train Acc: 86.57% | Val Loss: 3.3480 | Val Acc: 31.25%
[2025-12-13 14:54:59,328] INFO: No improvement in val loss for 33 epoch(s).
[2025-12-13 14:55:00,028] INFO: Epoch 35/1000 | Train Loss: 0.4033 | Train Acc: 85.83% | Val Loss: 3.2439 | Val Acc: 29.78%
[2025-12-13 14:55:00,028] INFO: No improvement in val loss for 34 epoch(s).
[2025-12-13 14:55:00,719] INFO: Epoch 36/1000 | Train Loss: 0.4057 | Train Acc: 86.24% | Val Loss: 3.2626 | Val Acc: 28.31%
[2025-12-13 14:55:00,719] INFO: No improvement in val loss for 35 epoch(s).
[2025-12-13 14:55:01,427] INFO: Epoch 37/1000 | Train Loss: 0.4110 | Train Acc: 86.40% | Val Loss: 3.0385 | Val Acc: 31.99%
[2025-12-13 14:55:01,427] INFO: No improvement in val loss for 36 epoch(s).
[2025-12-13 14:55:02,139] INFO: Epoch 38/1000 | Train Loss: 0.4267 | Train Acc: 85.29% | Val Loss: 3.1815 | Val Acc: 33.46%
[2025-12-13 14:55:02,139] INFO: No improvement in val loss for 37 epoch(s).
[2025-12-13 14:55:02,879] INFO: Epoch 39/1000 | Train Loss: 0.3893 | Train Acc: 86.73% | Val Loss: 3.2102 | Val Acc: 31.62%
[2025-12-13 14:55:02,879] INFO: No improvement in val loss for 38 epoch(s).
[2025-12-13 14:55:03,584] INFO: Epoch 40/1000 | Train Loss: 0.3949 | Train Acc: 86.77% | Val Loss: 3.0665 | Val Acc: 31.62%
[2025-12-13 14:55:03,585] INFO: No improvement in val loss for 39 epoch(s).
[2025-12-13 14:55:04,285] INFO: Epoch 41/1000 | Train Loss: 0.4050 | Train Acc: 86.57% | Val Loss: 2.9237 | Val Acc: 30.51%
[2025-12-13 14:55:04,286] INFO: No improvement in val loss for 40 epoch(s).
[2025-12-13 14:55:04,286] INFO: Early stopping triggered after 40 epochs with no improvement.
[2025-12-13 14:55:04,286] INFO: Word2Vec lifecycle event {'fname_or_handle': '/app/data/w2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-12-13T14:55:04.286341', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'saving'}
[2025-12-13 14:55:04,286] INFO: not storing attribute cum_table
[2025-12-13 14:55:04,309] INFO: saved /app/data/w2v.model
[2025-12-13 14:55:04,309] INFO: Word2Vec model saved to /app/data/w2v.model
[2025-12-13 14:55:04,309] INFO: ==== FULL DATASET TRAIN/VAL END ====
Running script: 03_evaluation.py
[2025-12-13 14:55:09,245] INFO: ==== EVALUATION START ====
[2025-12-13 14:55:11,268] INFO: Using device: cuda
[2025-12-13 14:55:11,311] INFO: GPU: NVIDIA GeForce GTX 1050 Ti, CUDA capability: 6.1
[2025-12-13 14:55:11,327] INFO: Loaded test dataset with 307 samples.
[2025-12-13 14:55:11,333] INFO: loading Word2Vec object from /app/data/w2v.model
[2025-12-13 14:55:11,349] INFO: loading wv recursively from /app/data/w2v.model.wv.* with mmap=None
[2025-12-13 14:55:11,350] INFO: setting ignored attribute cum_table to None
[2025-12-13 14:55:11,480] INFO: Word2Vec lifecycle event {'fname': '/app/data/w2v.model', 'datetime': '2025-12-13T14:55:11.477792', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-13 14:55:11,481] INFO: Word2Vec model loaded from /app/data/w2v.model with vocab size 7072 and vector size 64
[2025-12-13 14:55:17,462] INFO: LSTM model loaded from /app/data/lstm_model.pth and ready for evaluation.
[2025-12-13 14:55:17,463] INFO: Starting LSTM evaluation on test set...
[2025-12-13 14:55:17,793] INFO: LSTM Test Loss: 1.4641
[2025-12-13 14:55:17,794] INFO: LSTM Test Accuracy: 28.99%
[2025-12-13 14:55:17,795] INFO: LSTM F1-score (weighted): 0.2195
[2025-12-13 14:55:17,796] INFO: LSTM Confusion Matrix:
[[ 0  0  2  0 11]
 [ 0  0  1  9 25]
 [ 0  0  7 10 48]
 [ 0  0  5 15 86]
 [ 0  0  5 16 67]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 14:55:17,816] INFO: LSTM Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.00      0.00      0.00        35
           2       0.35      0.11      0.16        65
           3       0.30      0.14      0.19       106
           4       0.28      0.76      0.41        88

    accuracy                           0.29       307
   macro avg       0.19      0.20      0.15       307
weighted avg       0.26      0.29      0.22       307

[2025-12-13 14:55:17,817] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 14:55:17,818] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 14:55:17,861] INFO: Train dataset loaded: 2434 samples.
[2025-12-13 14:55:19,456] INFO: TF-IDF + Logistic Regression trained.
[2025-12-13 14:55:19,497] INFO: TF-IDF + Logistic Regression Test Accuracy: 36.81%
[2025-12-13 14:55:19,498] INFO: TF-IDF + Logistic Regression F1-score (weighted): 0.3350
[2025-12-13 14:55:19,504] INFO: TF-IDF + Logistic Regression Confusion Matrix:
[[ 0  2  4  2  5]
 [ 0  1 16 14  4]
 [ 0  4  9 33 19]
 [ 0  1 24 47 34]
 [ 0  0  8 24 56]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 14:55:19,530] INFO: TF-IDF Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.12      0.03      0.05        35
           2       0.15      0.14      0.14        65
           3       0.39      0.44      0.42       106
           4       0.47      0.64      0.54        88

    accuracy                           0.37       307
   macro avg       0.23      0.25      0.23       307
weighted avg       0.32      0.37      0.34       307

[2025-12-13 14:55:19,531] INFO: ==== EVALUATION END ====
=== Finished at Sat Dec 13 14:55:20 UTC 2025 ===
