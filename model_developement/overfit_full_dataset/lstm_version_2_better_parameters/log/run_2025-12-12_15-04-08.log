=== Starting pipeline at Sat Dec 13 15:33:59 UTC 2025 ===
Running script: 00_download_data.py
Downloading to /app/data/downloaded.zip ...
File downloaded successfully!
Running script: 01_data_preprocessing.py
ZIP extracted to: /app/extracted_zip
ERROR loading file.json: 'rating'
ERROR loading meta.json: 'str' object has no attribute 'get'
Base DF columns: Index(['text', 'rating'], dtype='object')
Consensus DF columns: Index(['text', 'rating'], dtype='object')
Base DF shape: (3737, 2)
Consensus DF shape: (2648, 2)
Number of rows filtered out from base_df because they exist in consensus: 615
Number of aggregated consensus texts (after averaging): 609
Consensus inference saved to: /app/data/inference.csv
Train/Val/Test split saved.
Running script: 02_train.py
[2025-12-13 15:34:11,077] INFO: ==== FULL DATASET TRAIN/VAL START ====
[2025-12-13 15:34:13,725] INFO: Using device: cuda
[2025-12-13 15:34:13,777] INFO: Train size: 2434, Val size: 272
[2025-12-13 15:34:13,818] INFO: collecting all words and their counts
[2025-12-13 15:34:13,819] INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
[2025-12-13 15:34:13,858] INFO: collected 24220 word types from a corpus of 120832 raw words and 2434 sentences
[2025-12-13 15:34:13,858] INFO: Creating a fresh vocabulary
[2025-12-13 15:34:14,014] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 24220 unique words (100.00% of original 24220, drops 0)', 'datetime': '2025-12-13T15:34:14.011660', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 15:34:14,014] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 120832 word corpus (100.00% of original 120832, drops 0)', 'datetime': '2025-12-13T15:34:14.014863', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 15:34:14,271] INFO: deleting the raw counts dictionary of 24220 items
[2025-12-13 15:34:14,272] INFO: sample=0.001 downsamples 19 most-common words
[2025-12-13 15:34:14,272] INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 101584.10254042316 word corpus (84.1%% of prior 120832)', 'datetime': '2025-12-13T15:34:14.272816', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 15:34:14,688] INFO: estimated required memory for 24220 words and 64 dimensions: 24510640 bytes
[2025-12-13 15:34:14,688] INFO: resetting layer weights
[2025-12-13 15:34:14,709] INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-13T15:34:14.709886', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}
[2025-12-13 15:34:14,710] INFO: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 24220 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-13T15:34:14.710296', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 15:34:14,859] INFO: EPOCH 0: training on 120832 raw words (101476 effective words) took 0.1s, 751451 effective words/s
[2025-12-13 15:34:14,982] INFO: EPOCH 1: training on 120832 raw words (101570 effective words) took 0.1s, 881275 effective words/s
[2025-12-13 15:34:15,101] INFO: EPOCH 2: training on 120832 raw words (101462 effective words) took 0.1s, 912263 effective words/s
[2025-12-13 15:34:15,234] INFO: EPOCH 3: training on 120832 raw words (101589 effective words) took 0.1s, 795071 effective words/s
[2025-12-13 15:34:15,368] INFO: EPOCH 4: training on 120832 raw words (101653 effective words) took 0.1s, 804768 effective words/s
[2025-12-13 15:34:15,368] INFO: Word2Vec lifecycle event {'msg': 'training on 604160 raw words (507750 effective words) took 0.7s, 771332 effective words/s', 'datetime': '2025-12-13T15:34:15.368739', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 15:34:15,369] INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=24220, vector_size=64, alpha=0.025>', 'datetime': '2025-12-13T15:34:15.369000', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}
[2025-12-13 15:34:15,369] INFO: Word2Vec trained. Vocab size: 24220 Embedding dim: 64
[2025-12-13 15:34:15,485] INFO: ==== CONFIG & HYPERPARAMETERS START ====
[2025-12-13 15:34:15,485] INFO: BATCH_SIZE : 16
[2025-12-13 15:34:15,485] INFO: DATA_DIR : /app/data
[2025-12-13 15:34:15,485] INFO: EARLY_STOP_PATIENCE : 40
[2025-12-13 15:34:15,485] INFO: FC_DROPOUT : 0.4
[2025-12-13 15:34:15,485] INFO: LEARNING_RATE : 0.001
[2025-12-13 15:34:15,485] INFO: LOG_FILE : /app/logs/run.log
[2025-12-13 15:34:15,485] INFO: LSTM_DROPOUT : 0.3
[2025-12-13 15:34:15,486] INFO: LSTM_HIDDEN_DIM : 128
[2025-12-13 15:34:15,486] INFO: MODEL_NAME : LSTM
[2025-12-13 15:34:15,486] INFO: MODEL_PATH : /app/data/lstm_model.pth
[2025-12-13 15:34:15,486] INFO: NUM_CLASSES : 5
[2025-12-13 15:34:15,486] INFO: NUM_EPOCHS : 1000
[2025-12-13 15:34:15,486] INFO: NUM_LSTM_LAYERS : 2
[2025-12-13 15:34:15,486] INFO: SEQ_LEN : 200
[2025-12-13 15:34:15,486] INFO: W2V_PATH : /app/data/w2v.model
[2025-12-13 15:34:15,486] INFO: WEIGHT_DECAY : 0.0001
[2025-12-13 15:34:15,487] INFO: WORD2_VEC_MIN_COUNT : 1
[2025-12-13 15:34:15,487] INFO: WORD2_VEC_VECTOR_SIZE: 64
[2025-12-13 15:34:15,487] INFO: ==== CONFIG & HYPERPARAMETERS END ====
[2025-12-13 15:34:21,724] INFO: ==== MODEL SUMMARY START ====
[2025-12-13 15:34:21,724] INFO: Model architecture:
[2025-12-13 15:34:21,724] INFO: MultiLayerLSTM(
(emb): Embedding(24221, 64, padding_idx=0)
(lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)
(dropout): Dropout(p=0.4, inplace=False)
(fc): Linear(in_features=128, out_features=5, bias=True)
)
[2025-12-13 15:34:21,725] INFO: Total parameters : 1,782,213
[2025-12-13 15:34:21,725] INFO: Trainable parameters : 1,782,213
[2025-12-13 15:34:21,725] INFO: Frozen parameters : 0
[2025-12-13 15:34:21,725] INFO: Embedding parameters : 1,550,144
[2025-12-13 15:34:21,725] INFO: ==== MODEL SUMMARY END ====
[2025-12-13 15:34:26,306] INFO: Epoch 1/1000 | Train Loss: 1.4564 | Train Acc: 32.05% | Val Loss: 1.4246 | Val Acc: 32.35%
[2025-12-13 15:34:26,349] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 15:34:28,816] INFO: Epoch 2/1000 | Train Loss: 1.4714 | Train Acc: 30.98% | Val Loss: 1.4355 | Val Acc: 32.35%
[2025-12-13 15:34:28,817] INFO: No improvement in val loss for 1 epoch(s).
[2025-12-13 15:34:31,296] INFO: Epoch 3/1000 | Train Loss: 1.4509 | Train Acc: 33.11% | Val Loss: 1.4294 | Val Acc: 32.35%
[2025-12-13 15:34:31,296] INFO: No improvement in val loss for 2 epoch(s).
[2025-12-13 15:34:33,759] INFO: Epoch 4/1000 | Train Loss: 1.4504 | Train Acc: 31.43% | Val Loss: 1.4257 | Val Acc: 32.35%
[2025-12-13 15:34:33,759] INFO: No improvement in val loss for 3 epoch(s).
[2025-12-13 15:34:36,229] INFO: Epoch 5/1000 | Train Loss: 1.4507 | Train Acc: 31.64% | Val Loss: 1.4285 | Val Acc: 32.35%
[2025-12-13 15:34:36,230] INFO: No improvement in val loss for 4 epoch(s).
[2025-12-13 15:34:38,753] INFO: Epoch 6/1000 | Train Loss: 1.4346 | Train Acc: 33.77% | Val Loss: 1.4255 | Val Acc: 35.66%
[2025-12-13 15:34:38,753] INFO: No improvement in val loss for 5 epoch(s).
[2025-12-13 15:34:41,254] INFO: Epoch 7/1000 | Train Loss: 1.3987 | Train Acc: 37.47% | Val Loss: 1.4032 | Val Acc: 39.34%
[2025-12-13 15:34:41,282] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 15:34:43,767] INFO: Epoch 8/1000 | Train Loss: 1.3586 | Train Acc: 40.88% | Val Loss: 1.3950 | Val Acc: 34.19%
[2025-12-13 15:34:43,790] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 15:34:46,273] INFO: Epoch 9/1000 | Train Loss: 1.3717 | Train Acc: 37.76% | Val Loss: 1.3882 | Val Acc: 33.46%
[2025-12-13 15:34:46,298] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 15:34:48,776] INFO: Epoch 10/1000 | Train Loss: 1.3362 | Train Acc: 40.26% | Val Loss: 1.3827 | Val Acc: 36.76%
[2025-12-13 15:34:48,801] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 15:34:51,281] INFO: Epoch 11/1000 | Train Loss: 1.3307 | Train Acc: 41.21% | Val Loss: 1.4032 | Val Acc: 38.97%
[2025-12-13 15:34:51,281] INFO: No improvement in val loss for 6 epoch(s).
[2025-12-13 15:34:53,764] INFO: Epoch 12/1000 | Train Loss: 1.2809 | Train Acc: 43.92% | Val Loss: 1.4159 | Val Acc: 37.13%
[2025-12-13 15:34:53,765] INFO: No improvement in val loss for 7 epoch(s).
[2025-12-13 15:34:56,242] INFO: Epoch 13/1000 | Train Loss: 1.2259 | Train Acc: 46.75% | Val Loss: 1.4396 | Val Acc: 37.13%
[2025-12-13 15:34:56,242] INFO: No improvement in val loss for 8 epoch(s).
[2025-12-13 15:34:58,724] INFO: Epoch 14/1000 | Train Loss: 1.1263 | Train Acc: 54.31% | Val Loss: 1.6232 | Val Acc: 36.76%
[2025-12-13 15:34:58,724] INFO: No improvement in val loss for 9 epoch(s).
[2025-12-13 15:35:01,212] INFO: Epoch 15/1000 | Train Loss: 1.0375 | Train Acc: 57.52% | Val Loss: 1.5095 | Val Acc: 36.76%
[2025-12-13 15:35:01,212] INFO: No improvement in val loss for 10 epoch(s).
[2025-12-13 15:35:03,677] INFO: Epoch 16/1000 | Train Loss: 0.9708 | Train Acc: 61.46% | Val Loss: 1.7718 | Val Acc: 36.40%
[2025-12-13 15:35:03,677] INFO: No improvement in val loss for 11 epoch(s).
[2025-12-13 15:35:06,157] INFO: Epoch 17/1000 | Train Loss: 0.8187 | Train Acc: 68.08% | Val Loss: 1.8608 | Val Acc: 37.13%
[2025-12-13 15:35:06,157] INFO: No improvement in val loss for 12 epoch(s).
[2025-12-13 15:35:08,627] INFO: Epoch 18/1000 | Train Loss: 0.7759 | Train Acc: 70.34% | Val Loss: 1.9088 | Val Acc: 37.13%
[2025-12-13 15:35:08,627] INFO: No improvement in val loss for 13 epoch(s).
[2025-12-13 15:35:11,114] INFO: Epoch 19/1000 | Train Loss: 0.7419 | Train Acc: 70.75% | Val Loss: 2.0911 | Val Acc: 38.97%
[2025-12-13 15:35:11,114] INFO: No improvement in val loss for 14 epoch(s).
[2025-12-13 15:35:13,589] INFO: Epoch 20/1000 | Train Loss: 0.6724 | Train Acc: 73.42% | Val Loss: 2.1920 | Val Acc: 35.66%
[2025-12-13 15:35:13,590] INFO: No improvement in val loss for 15 epoch(s).
[2025-12-13 15:35:16,062] INFO: Epoch 21/1000 | Train Loss: 0.6195 | Train Acc: 76.87% | Val Loss: 2.4829 | Val Acc: 36.03%
[2025-12-13 15:35:16,062] INFO: No improvement in val loss for 16 epoch(s).
[2025-12-13 15:35:18,528] INFO: Epoch 22/1000 | Train Loss: 0.6084 | Train Acc: 76.91% | Val Loss: 2.3032 | Val Acc: 36.03%
[2025-12-13 15:35:18,528] INFO: No improvement in val loss for 17 epoch(s).
[2025-12-13 15:35:21,003] INFO: Epoch 23/1000 | Train Loss: 0.5506 | Train Acc: 80.16% | Val Loss: 2.4246 | Val Acc: 37.50%
[2025-12-13 15:35:21,003] INFO: No improvement in val loss for 18 epoch(s).
[2025-12-13 15:35:23,470] INFO: Epoch 24/1000 | Train Loss: 0.5338 | Train Acc: 81.39% | Val Loss: 2.3943 | Val Acc: 36.76%
[2025-12-13 15:35:23,470] INFO: No improvement in val loss for 19 epoch(s).
[2025-12-13 15:35:25,963] INFO: Epoch 25/1000 | Train Loss: 0.5119 | Train Acc: 82.87% | Val Loss: 2.3647 | Val Acc: 34.93%
[2025-12-13 15:35:25,963] INFO: No improvement in val loss for 20 epoch(s).
[2025-12-13 15:35:28,604] INFO: Epoch 26/1000 | Train Loss: 0.4411 | Train Acc: 86.15% | Val Loss: 2.7677 | Val Acc: 35.66%
[2025-12-13 15:35:28,604] INFO: No improvement in val loss for 21 epoch(s).
[2025-12-13 15:35:31,195] INFO: Epoch 27/1000 | Train Loss: 0.4222 | Train Acc: 87.30% | Val Loss: 2.5524 | Val Acc: 35.29%
[2025-12-13 15:35:31,195] INFO: No improvement in val loss for 22 epoch(s).
[2025-12-13 15:35:33,898] INFO: Epoch 28/1000 | Train Loss: 0.4423 | Train Acc: 85.91% | Val Loss: 2.5054 | Val Acc: 37.87%
[2025-12-13 15:35:33,898] INFO: No improvement in val loss for 23 epoch(s).
[2025-12-13 15:35:36,376] INFO: Epoch 29/1000 | Train Loss: 0.5028 | Train Acc: 82.58% | Val Loss: 2.7904 | Val Acc: 36.03%
[2025-12-13 15:35:36,376] INFO: No improvement in val loss for 24 epoch(s).
[2025-12-13 15:35:38,848] INFO: Epoch 30/1000 | Train Loss: 0.4093 | Train Acc: 87.06% | Val Loss: 2.7507 | Val Acc: 37.87%
[2025-12-13 15:35:38,848] INFO: No improvement in val loss for 25 epoch(s).
[2025-12-13 15:35:41,330] INFO: Epoch 31/1000 | Train Loss: 0.3758 | Train Acc: 88.29% | Val Loss: 2.8876 | Val Acc: 36.76%
[2025-12-13 15:35:41,331] INFO: No improvement in val loss for 26 epoch(s).
[2025-12-13 15:35:43,817] INFO: Epoch 32/1000 | Train Loss: 0.3331 | Train Acc: 89.73% | Val Loss: 2.9938 | Val Acc: 36.03%
[2025-12-13 15:35:43,818] INFO: No improvement in val loss for 27 epoch(s).
[2025-12-13 15:35:46,307] INFO: Epoch 33/1000 | Train Loss: 0.3446 | Train Acc: 89.52% | Val Loss: 2.9001 | Val Acc: 35.66%
[2025-12-13 15:35:46,307] INFO: No improvement in val loss for 28 epoch(s).
[2025-12-13 15:35:48,785] INFO: Epoch 34/1000 | Train Loss: 0.3327 | Train Acc: 89.81% | Val Loss: 3.0526 | Val Acc: 35.29%
[2025-12-13 15:35:48,785] INFO: No improvement in val loss for 29 epoch(s).
[2025-12-13 15:35:51,272] INFO: Epoch 35/1000 | Train Loss: 0.3149 | Train Acc: 90.26% | Val Loss: 2.8891 | Val Acc: 36.03%
[2025-12-13 15:35:51,272] INFO: No improvement in val loss for 30 epoch(s).
[2025-12-13 15:35:53,759] INFO: Epoch 36/1000 | Train Loss: 0.3186 | Train Acc: 90.47% | Val Loss: 3.1131 | Val Acc: 37.13%
[2025-12-13 15:35:53,759] INFO: No improvement in val loss for 31 epoch(s).
[2025-12-13 15:35:56,241] INFO: Epoch 37/1000 | Train Loss: 0.3369 | Train Acc: 89.81% | Val Loss: 2.9851 | Val Acc: 33.09%
[2025-12-13 15:35:56,242] INFO: No improvement in val loss for 32 epoch(s).
[2025-12-13 15:35:58,727] INFO: Epoch 38/1000 | Train Loss: 0.3371 | Train Acc: 89.93% | Val Loss: 3.1534 | Val Acc: 35.66%
[2025-12-13 15:35:58,728] INFO: No improvement in val loss for 33 epoch(s).
[2025-12-13 15:36:01,211] INFO: Epoch 39/1000 | Train Loss: 0.3495 | Train Acc: 89.40% | Val Loss: 3.0911 | Val Acc: 33.09%
[2025-12-13 15:36:01,211] INFO: No improvement in val loss for 34 epoch(s).
[2025-12-13 15:36:03,694] INFO: Epoch 40/1000 | Train Loss: 0.3610 | Train Acc: 89.19% | Val Loss: 2.6183 | Val Acc: 32.72%
[2025-12-13 15:36:03,695] INFO: No improvement in val loss for 35 epoch(s).
[2025-12-13 15:36:06,192] INFO: Epoch 41/1000 | Train Loss: 0.3275 | Train Acc: 90.35% | Val Loss: 3.1678 | Val Acc: 32.72%
[2025-12-13 15:36:06,192] INFO: No improvement in val loss for 36 epoch(s).
[2025-12-13 15:36:08,689] INFO: Epoch 42/1000 | Train Loss: 0.3016 | Train Acc: 90.96% | Val Loss: 3.2001 | Val Acc: 34.56%
[2025-12-13 15:36:08,690] INFO: No improvement in val loss for 37 epoch(s).
[2025-12-13 15:36:11,176] INFO: Epoch 43/1000 | Train Loss: 0.3123 | Train Acc: 90.18% | Val Loss: 2.6766 | Val Acc: 37.13%
[2025-12-13 15:36:11,176] INFO: No improvement in val loss for 38 epoch(s).
[2025-12-13 15:36:13,659] INFO: Epoch 44/1000 | Train Loss: 0.3447 | Train Acc: 89.19% | Val Loss: 2.7885 | Val Acc: 37.87%
[2025-12-13 15:36:13,659] INFO: No improvement in val loss for 39 epoch(s).
[2025-12-13 15:36:16,161] INFO: Epoch 45/1000 | Train Loss: 0.3401 | Train Acc: 89.44% | Val Loss: 2.8126 | Val Acc: 35.66%
[2025-12-13 15:36:16,161] INFO: No improvement in val loss for 40 epoch(s).
[2025-12-13 15:36:16,161] INFO: Early stopping triggered after 40 epochs with no improvement.
[2025-12-13 15:36:16,161] INFO: Word2Vec lifecycle event {'fname_or_handle': '/app/data/w2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-12-13T15:36:16.161764', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'saving'}
[2025-12-13 15:36:16,163] INFO: not storing attribute cum_table
[2025-12-13 15:36:16,235] INFO: saved /app/data/w2v.model
[2025-12-13 15:36:16,235] INFO: Word2Vec model saved to /app/data/w2v.model
[2025-12-13 15:36:16,235] INFO: ==== FULL DATASET TRAIN/VAL END ====
Running script: 03_evaluation.py
[2025-12-13 15:36:21,474] INFO: ==== EVALUATION START ====
[2025-12-13 15:36:23,549] INFO: Using device: cuda
[2025-12-13 15:36:23,586] INFO: GPU: NVIDIA GeForce GTX 1050 Ti, CUDA capability: 6.1
[2025-12-13 15:36:23,598] INFO: Loaded test dataset with 307 samples.
[2025-12-13 15:36:23,603] INFO: loading Word2Vec object from /app/data/w2v.model
[2025-12-13 15:36:23,637] INFO: loading wv recursively from /app/data/w2v.model.wv.* with mmap=None
[2025-12-13 15:36:23,638] INFO: setting ignored attribute cum_table to None
[2025-12-13 15:36:24,049] INFO: Word2Vec lifecycle event {'fname': '/app/data/w2v.model', 'datetime': '2025-12-13T15:36:24.046837', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-13 15:36:24,050] INFO: Word2Vec model loaded from /app/data/w2v.model with vocab size 24220 and vector size 64
[2025-12-13 15:36:30,258] INFO: LSTM model loaded from /app/data/lstm_model.pth and ready for evaluation.
[2025-12-13 15:36:30,259] INFO: Starting LSTM evaluation on test set...
[2025-12-13 15:36:30,739] INFO: LSTM Test Loss: 1.3492
[2025-12-13 15:36:30,739] INFO: LSTM Test Accuracy: 41.37%
[2025-12-13 15:36:30,740] INFO: LSTM F1-score (weighted): 0.3775
[2025-12-13 15:36:30,742] INFO: LSTM Confusion Matrix:
[[ 0 0 7 3 3]
[ 0 0 14 14 7]
[ 0 0 23 31 11]
[ 0 0 21 60 25]
[ 0 0 4 40 44]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
_warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
_warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
_warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 15:36:30,759] INFO: LSTM Classification Report:
precision recall f1-score support

0 0.00 0.00 0.00 13
1 0.00 0.00 0.00 35
2 0.33 0.35 0.34 65
3 0.41 0.57 0.47 106
4 0.49 0.50 0.49 88

accuracy 0.41 307
macro avg 0.25 0.28 0.26 307
weighted avg 0.35 0.41 0.38 307

[2025-12-13 15:36:30,759] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 15:36:30,760] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 15:36:30,801] INFO: Train dataset loaded: 2434 samples.
[2025-12-13 15:36:32,516] INFO: TF-IDF + Logistic Regression trained.
[2025-12-13 15:36:32,555] INFO: TF-IDF + Logistic Regression Test Accuracy: 36.81%
[2025-12-13 15:36:32,558] INFO: TF-IDF + Logistic Regression F1-score (weighted): 0.3350
[2025-12-13 15:36:32,560] INFO: TF-IDF + Logistic Regression Confusion Matrix:
[[ 0 2 4 2 5]
[ 0 1 16 14 4]
[ 0 4 9 33 19]
[ 0 1 24 47 34]
[ 0 0 8 24 56]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
_warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
_warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
_warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 15:36:32,581] INFO: TF-IDF Classification Report:
precision recall f1-score support

0 0.00 0.00 0.00 13
1 0.12 0.03 0.05 35
2 0.15 0.14 0.14 65
3 0.39 0.44 0.42 106
4 0.47 0.64 0.54 88

accuracy 0.37 307
macro avg 0.23 0.25 0.23 307
weighted avg 0.32 0.37 0.34 307

[2025-12-13 15:36:32,582] INFO: ==== EVALUATION END ====
=== Finished at Sat Dec 13 15:36:33 UTC 2025 ===