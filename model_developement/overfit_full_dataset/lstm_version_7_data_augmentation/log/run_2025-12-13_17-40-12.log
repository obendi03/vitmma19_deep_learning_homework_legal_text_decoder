=== Starting pipeline at Sat Dec 13 17:40:12 UTC 2025 ===
Running script: 00_download_data.py
Downloading to /app/data/downloaded.zip ...
File downloaded successfully!
Running script: 01_data_preprocessing.py
ZIP extracted to: /app/extracted_zip
ERROR loading file.json: 'rating'
ERROR loading meta.json: 'str' object has no attribute 'get'
Base DF columns: Index(['text', 'rating'], dtype='object')
Consensus DF columns: Index(['text', 'rating'], dtype='object')
Base DF shape: (3737, 2)
Consensus DF shape: (2648, 2)
Number of rows filtered out from base_df because they exist in consensus: 615
Number of aggregated consensus texts (after averaging): 609
Consensus inference saved to: /app/data/inference.csv
Train/Val/Test split saved.
Running script: 02_train.py
[2025-12-13 17:40:24,894] INFO: ==== FULL DATASET TRAIN/VAL START ====
[2025-12-13 17:40:27,577] INFO: Using device: cuda
Alkalmazott jogi augmentáció...
Augmentáció kész!
[2025-12-13 17:40:27,712] INFO: Train size: 2434, Val size: 272
[2025-12-13 17:40:27,754] INFO: collecting all words and their counts
[2025-12-13 17:40:27,754] INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
[2025-12-13 17:40:27,793] INFO: collected 23186 word types from a corpus of 120838 raw words and 2434 sentences
[2025-12-13 17:40:27,794] INFO: Creating a fresh vocabulary
[2025-12-13 17:40:27,959] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 23186 unique words (100.00% of original 23186, drops 0)', 'datetime': '2025-12-13T17:40:27.941463', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 17:40:27,959] INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 120838 word corpus (100.00% of original 120838, drops 0)', 'datetime': '2025-12-13T17:40:27.959868', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 17:40:28,207] INFO: deleting the raw counts dictionary of 23186 items
[2025-12-13 17:40:28,209] INFO: sample=0.001 downsamples 21 most-common words
[2025-12-13 17:40:28,210] INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 100257.6633468385 word corpus (83.0%% of prior 120838)', 'datetime': '2025-12-13T17:40:28.210138', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}
[2025-12-13 17:40:28,613] INFO: estimated required memory for 23186 words and 64 dimensions: 23464232 bytes
[2025-12-13 17:40:28,613] INFO: resetting layer weights
[2025-12-13 17:40:28,637] INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-12-13T17:40:28.637847', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'build_vocab'}
[2025-12-13 17:40:28,638] INFO: Word2Vec lifecycle event {'msg': 'training model with 4 workers on 23186 vocabulary and 64 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-12-13T17:40:28.638270', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 17:40:28,791] INFO: EPOCH 0: training on 120838 raw words (100167 effective words) took 0.1s, 701157 effective words/s
[2025-12-13 17:40:28,911] INFO: EPOCH 1: training on 120838 raw words (100287 effective words) took 0.1s, 877930 effective words/s
[2025-12-13 17:40:29,030] INFO: EPOCH 2: training on 120838 raw words (100169 effective words) took 0.1s, 879936 effective words/s
[2025-12-13 17:40:29,148] INFO: EPOCH 3: training on 120838 raw words (100323 effective words) took 0.1s, 911714 effective words/s
[2025-12-13 17:40:29,269] INFO: EPOCH 4: training on 120838 raw words (100168 effective words) took 0.1s, 876315 effective words/s
[2025-12-13 17:40:29,269] INFO: Word2Vec lifecycle event {'msg': 'training on 604190 raw words (501114 effective words) took 0.6s, 797604 effective words/s', 'datetime': '2025-12-13T17:40:29.269578', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'train'}
[2025-12-13 17:40:29,269] INFO: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=23186, vector_size=64, alpha=0.025>', 'datetime': '2025-12-13T17:40:29.269755', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}
[2025-12-13 17:40:29,270] INFO: Word2Vec trained. Vocab size: 23186 Embedding dim: 64
[2025-12-13 17:40:29,555] INFO: ==== CONFIG & HYPERPARAMETERS START ====
[2025-12-13 17:40:29,555] INFO: AUGMENTATION        : True
[2025-12-13 17:40:29,555] INFO: AUG_PROB            : 0.15
[2025-12-13 17:40:29,555] INFO: BATCH_SIZE          : 32
[2025-12-13 17:40:29,555] INFO: BIDIRECTIONAL       : False
[2025-12-13 17:40:29,555] INFO: DATA_DIR            : /app/data
[2025-12-13 17:40:29,556] INFO: EARLY_STOP_PATIENCE : 40
[2025-12-13 17:40:29,556] INFO: FC_DROPOUT          : 0.4
[2025-12-13 17:40:29,556] INFO: LEARNING_RATE       : 0.001
[2025-12-13 17:40:29,556] INFO: LOG_FILE            : /app/logs/run.log
[2025-12-13 17:40:29,556] INFO: LSTM_DROPOUT        : 0.3
[2025-12-13 17:40:29,556] INFO: LSTM_HIDDEN_DIM     : 128
[2025-12-13 17:40:29,557] INFO: MODEL_NAME          : LSTM
[2025-12-13 17:40:29,557] INFO: MODEL_PATH          : /app/data/lstm_model.pth
[2025-12-13 17:40:29,557] INFO: NUM_CLASSES         : 5
[2025-12-13 17:40:29,557] INFO: NUM_EPOCHS          : 1000
[2025-12-13 17:40:29,557] INFO: NUM_LSTM_LAYERS     : 2
[2025-12-13 17:40:29,557] INFO: SEQ_LEN             : 200
[2025-12-13 17:40:29,558] INFO: W2V_PATH            : /app/data/w2v.model
[2025-12-13 17:40:29,558] INFO: WEIGHT_DECAY        : 0.0001
[2025-12-13 17:40:29,558] INFO: WORD2_VEC_MIN_COUNT : 1
[2025-12-13 17:40:29,558] INFO: WORD2_VEC_VECTOR_SIZE: 64
[2025-12-13 17:40:29,558] INFO: ==== CONFIG & HYPERPARAMETERS END ====
[2025-12-13 17:40:43,303] INFO: ==== MODEL SUMMARY START ====
[2025-12-13 17:40:43,303] INFO: Model architecture:
[2025-12-13 17:40:43,303] INFO: MultiLayerLSTM(
  (emb): Embedding(23187, 64, padding_idx=0)
  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.3)
  (attention): Linear(in_features=128, out_features=1, bias=True)
  (dropout): Dropout(p=0.4, inplace=False)
  (fc): Linear(in_features=128, out_features=5, bias=True)
)
[2025-12-13 17:40:43,304] INFO: Total parameters      : 1,716,166
[2025-12-13 17:40:43,304] INFO: Trainable parameters  : 1,716,166
[2025-12-13 17:40:43,304] INFO: Frozen parameters     : 0
[2025-12-13 17:40:43,304] INFO: Embedding parameters  : 1,483,968
[2025-12-13 17:40:43,304] INFO: ==== MODEL SUMMARY END ====
[2025-12-13 17:40:51,481] INFO: Epoch 1/1000 | Train Loss: 1.3535 | Train Acc: 39.85% | Val Loss: 1.4370 | Val Acc: 37.13%
[2025-12-13 17:40:51,557] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 17:40:53,604] INFO: Epoch 2/1000 | Train Loss: 1.2918 | Train Acc: 43.39% | Val Loss: 1.4094 | Val Acc: 37.50%
[2025-12-13 17:40:53,628] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 17:40:55,676] INFO: Epoch 3/1000 | Train Loss: 1.2993 | Train Acc: 42.73% | Val Loss: 1.3806 | Val Acc: 38.24%
[2025-12-13 17:40:55,698] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 17:40:57,745] INFO: Epoch 4/1000 | Train Loss: 1.2763 | Train Acc: 43.84% | Val Loss: 1.4190 | Val Acc: 36.40%
[2025-12-13 17:40:57,745] INFO: No improvement in val loss for 1 epoch(s).
[2025-12-13 17:40:59,836] INFO: Epoch 5/1000 | Train Loss: 1.2111 | Train Acc: 47.86% | Val Loss: 1.3559 | Val Acc: 37.50%
[2025-12-13 17:40:59,861] INFO: Validation loss improved. Model saved to /app/data/lstm_model.pth
[2025-12-13 17:41:01,911] INFO: Epoch 6/1000 | Train Loss: 0.9517 | Train Acc: 60.23% | Val Loss: 1.5747 | Val Acc: 38.24%
[2025-12-13 17:41:01,912] INFO: No improvement in val loss for 2 epoch(s).
[2025-12-13 17:41:03,964] INFO: Epoch 7/1000 | Train Loss: 0.7945 | Train Acc: 67.09% | Val Loss: 1.8785 | Val Acc: 39.71%
[2025-12-13 17:41:03,964] INFO: No improvement in val loss for 3 epoch(s).
[2025-12-13 17:41:06,015] INFO: Epoch 8/1000 | Train Loss: 0.6956 | Train Acc: 72.72% | Val Loss: 2.1040 | Val Acc: 36.03%
[2025-12-13 17:41:06,016] INFO: No improvement in val loss for 4 epoch(s).
[2025-12-13 17:41:08,064] INFO: Epoch 9/1000 | Train Loss: 0.6428 | Train Acc: 75.23% | Val Loss: 2.1686 | Val Acc: 38.24%
[2025-12-13 17:41:08,065] INFO: No improvement in val loss for 5 epoch(s).
[2025-12-13 17:41:10,115] INFO: Epoch 10/1000 | Train Loss: 0.5486 | Train Acc: 80.44% | Val Loss: 2.4479 | Val Acc: 40.44%
[2025-12-13 17:41:10,115] INFO: No improvement in val loss for 6 epoch(s).
[2025-12-13 17:41:12,171] INFO: Epoch 11/1000 | Train Loss: 0.4847 | Train Acc: 83.36% | Val Loss: 2.6108 | Val Acc: 39.34%
[2025-12-13 17:41:12,172] INFO: No improvement in val loss for 7 epoch(s).
[2025-12-13 17:41:14,295] INFO: Epoch 12/1000 | Train Loss: 0.4081 | Train Acc: 86.24% | Val Loss: 2.7879 | Val Acc: 32.35%
[2025-12-13 17:41:14,296] INFO: No improvement in val loss for 8 epoch(s).
[2025-12-13 17:41:16,354] INFO: Epoch 13/1000 | Train Loss: 0.3608 | Train Acc: 88.17% | Val Loss: 2.8873 | Val Acc: 40.07%
[2025-12-13 17:41:16,354] INFO: No improvement in val loss for 9 epoch(s).
[2025-12-13 17:41:18,411] INFO: Epoch 14/1000 | Train Loss: 0.2911 | Train Acc: 90.76% | Val Loss: 3.1546 | Val Acc: 37.50%
[2025-12-13 17:41:18,411] INFO: No improvement in val loss for 10 epoch(s).
[2025-12-13 17:41:20,462] INFO: Epoch 15/1000 | Train Loss: 0.2861 | Train Acc: 90.39% | Val Loss: 3.1837 | Val Acc: 38.24%
[2025-12-13 17:41:20,462] INFO: No improvement in val loss for 11 epoch(s).
[2025-12-13 17:41:22,515] INFO: Epoch 16/1000 | Train Loss: 0.2357 | Train Acc: 92.73% | Val Loss: 3.3116 | Val Acc: 39.71%
[2025-12-13 17:41:22,515] INFO: No improvement in val loss for 12 epoch(s).
[2025-12-13 17:41:24,565] INFO: Epoch 17/1000 | Train Loss: 0.1912 | Train Acc: 94.21% | Val Loss: 3.7468 | Val Acc: 36.40%
[2025-12-13 17:41:24,565] INFO: No improvement in val loss for 13 epoch(s).
[2025-12-13 17:41:26,615] INFO: Epoch 18/1000 | Train Loss: 0.1803 | Train Acc: 95.23% | Val Loss: 3.7478 | Val Acc: 39.34%
[2025-12-13 17:41:26,615] INFO: No improvement in val loss for 14 epoch(s).
[2025-12-13 17:41:28,690] INFO: Epoch 19/1000 | Train Loss: 0.1866 | Train Acc: 94.58% | Val Loss: 4.0381 | Val Acc: 35.29%
[2025-12-13 17:41:28,691] INFO: No improvement in val loss for 15 epoch(s).
[2025-12-13 17:41:30,762] INFO: Epoch 20/1000 | Train Loss: 0.2024 | Train Acc: 93.76% | Val Loss: 4.0423 | Val Acc: 33.82%
[2025-12-13 17:41:30,762] INFO: No improvement in val loss for 16 epoch(s).
[2025-12-13 17:41:32,818] INFO: Epoch 21/1000 | Train Loss: 0.1563 | Train Acc: 95.65% | Val Loss: 4.2975 | Val Acc: 36.03%
[2025-12-13 17:41:32,818] INFO: No improvement in val loss for 17 epoch(s).
[2025-12-13 17:41:34,881] INFO: Epoch 22/1000 | Train Loss: 0.1268 | Train Acc: 96.34% | Val Loss: 4.3558 | Val Acc: 35.29%
[2025-12-13 17:41:34,882] INFO: No improvement in val loss for 18 epoch(s).
[2025-12-13 17:41:36,935] INFO: Epoch 23/1000 | Train Loss: 0.1381 | Train Acc: 96.10% | Val Loss: 4.7657 | Val Acc: 33.09%
[2025-12-13 17:41:36,935] INFO: No improvement in val loss for 19 epoch(s).
[2025-12-13 17:41:38,986] INFO: Epoch 24/1000 | Train Loss: 0.1207 | Train Acc: 97.12% | Val Loss: 4.8440 | Val Acc: 34.56%
[2025-12-13 17:41:38,986] INFO: No improvement in val loss for 20 epoch(s).
[2025-12-13 17:41:41,041] INFO: Epoch 25/1000 | Train Loss: 0.1160 | Train Acc: 96.88% | Val Loss: 4.2611 | Val Acc: 37.50%
[2025-12-13 17:41:41,041] INFO: No improvement in val loss for 21 epoch(s).
[2025-12-13 17:41:43,096] INFO: Epoch 26/1000 | Train Loss: 0.1375 | Train Acc: 96.18% | Val Loss: 3.9841 | Val Acc: 40.07%
[2025-12-13 17:41:43,096] INFO: No improvement in val loss for 22 epoch(s).
[2025-12-13 17:41:45,176] INFO: Epoch 27/1000 | Train Loss: 0.1216 | Train Acc: 96.55% | Val Loss: 4.7688 | Val Acc: 33.09%
[2025-12-13 17:41:45,176] INFO: No improvement in val loss for 23 epoch(s).
[2025-12-13 17:41:47,228] INFO: Epoch 28/1000 | Train Loss: 0.1289 | Train Acc: 96.26% | Val Loss: 4.5472 | Val Acc: 36.03%
[2025-12-13 17:41:47,228] INFO: No improvement in val loss for 24 epoch(s).
[2025-12-13 17:41:49,279] INFO: Epoch 29/1000 | Train Loss: 0.1010 | Train Acc: 97.49% | Val Loss: 4.7688 | Val Acc: 33.09%
[2025-12-13 17:41:49,279] INFO: No improvement in val loss for 25 epoch(s).
[2025-12-13 17:41:51,329] INFO: Epoch 30/1000 | Train Loss: 0.0726 | Train Acc: 98.19% | Val Loss: 4.7676 | Val Acc: 33.82%
[2025-12-13 17:41:51,329] INFO: No improvement in val loss for 26 epoch(s).
[2025-12-13 17:41:53,380] INFO: Epoch 31/1000 | Train Loss: 0.0707 | Train Acc: 98.36% | Val Loss: 4.7039 | Val Acc: 35.66%
[2025-12-13 17:41:53,380] INFO: No improvement in val loss for 27 epoch(s).
[2025-12-13 17:41:55,437] INFO: Epoch 32/1000 | Train Loss: 0.0655 | Train Acc: 98.40% | Val Loss: 5.4589 | Val Acc: 36.03%
[2025-12-13 17:41:55,437] INFO: No improvement in val loss for 28 epoch(s).
[2025-12-13 17:41:57,490] INFO: Epoch 33/1000 | Train Loss: 0.1092 | Train Acc: 97.17% | Val Loss: 4.4993 | Val Acc: 33.09%
[2025-12-13 17:41:57,490] INFO: No improvement in val loss for 29 epoch(s).
[2025-12-13 17:41:59,565] INFO: Epoch 34/1000 | Train Loss: 0.1295 | Train Acc: 95.89% | Val Loss: 4.3379 | Val Acc: 35.29%
[2025-12-13 17:41:59,566] INFO: No improvement in val loss for 30 epoch(s).
[2025-12-13 17:42:01,619] INFO: Epoch 35/1000 | Train Loss: 0.1011 | Train Acc: 97.21% | Val Loss: 4.8430 | Val Acc: 34.93%
[2025-12-13 17:42:01,619] INFO: No improvement in val loss for 31 epoch(s).
[2025-12-13 17:42:03,676] INFO: Epoch 36/1000 | Train Loss: 0.0840 | Train Acc: 97.25% | Val Loss: 4.4711 | Val Acc: 37.87%
[2025-12-13 17:42:03,677] INFO: No improvement in val loss for 32 epoch(s).
[2025-12-13 17:42:05,737] INFO: Epoch 37/1000 | Train Loss: 0.0708 | Train Acc: 97.99% | Val Loss: 4.6933 | Val Acc: 36.40%
[2025-12-13 17:42:05,737] INFO: No improvement in val loss for 33 epoch(s).
[2025-12-13 17:42:07,798] INFO: Epoch 38/1000 | Train Loss: 0.0469 | Train Acc: 98.97% | Val Loss: 4.8884 | Val Acc: 37.13%
[2025-12-13 17:42:07,798] INFO: No improvement in val loss for 34 epoch(s).
[2025-12-13 17:42:09,854] INFO: Epoch 39/1000 | Train Loss: 0.0327 | Train Acc: 99.10% | Val Loss: 4.9652 | Val Acc: 37.50%
[2025-12-13 17:42:09,855] INFO: No improvement in val loss for 35 epoch(s).
[2025-12-13 17:42:11,947] INFO: Epoch 40/1000 | Train Loss: 0.0331 | Train Acc: 99.18% | Val Loss: 5.9437 | Val Acc: 31.62%
[2025-12-13 17:42:11,948] INFO: No improvement in val loss for 36 epoch(s).
[2025-12-13 17:42:14,034] INFO: Epoch 41/1000 | Train Loss: 0.0878 | Train Acc: 97.37% | Val Loss: 5.4091 | Val Acc: 36.03%
[2025-12-13 17:42:14,034] INFO: No improvement in val loss for 37 epoch(s).
[2025-12-13 17:42:16,163] INFO: Epoch 42/1000 | Train Loss: 0.1199 | Train Acc: 96.75% | Val Loss: 4.6621 | Val Acc: 34.19%
[2025-12-13 17:42:16,163] INFO: No improvement in val loss for 38 epoch(s).
[2025-12-13 17:42:18,345] INFO: Epoch 43/1000 | Train Loss: 0.0900 | Train Acc: 97.49% | Val Loss: 4.3483 | Val Acc: 35.66%
[2025-12-13 17:42:18,346] INFO: No improvement in val loss for 39 epoch(s).
[2025-12-13 17:42:20,464] INFO: Epoch 44/1000 | Train Loss: 0.0470 | Train Acc: 98.85% | Val Loss: 5.0803 | Val Acc: 36.40%
[2025-12-13 17:42:20,465] INFO: No improvement in val loss for 40 epoch(s).
[2025-12-13 17:42:20,465] INFO: Early stopping triggered after 40 epochs with no improvement.
[2025-12-13 17:42:20,465] INFO: Word2Vec lifecycle event {'fname_or_handle': '/app/data/w2v.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-12-13T17:42:20.465314', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'saving'}
[2025-12-13 17:42:20,465] INFO: not storing attribute cum_table
[2025-12-13 17:42:20,539] INFO: saved /app/data/w2v.model
[2025-12-13 17:42:20,539] INFO: Word2Vec model saved to /app/data/w2v.model
[2025-12-13 17:42:20,539] INFO: ==== FULL DATASET TRAIN/VAL END ====
Running script: 03_evaluation.py
[2025-12-13 17:42:26,347] INFO: ==== EVALUATION START ====
[2025-12-13 17:42:28,465] INFO: Using device: cuda
[2025-12-13 17:42:28,544] INFO: GPU: NVIDIA GeForce GTX 1050 Ti, CUDA capability: 6.1
[2025-12-13 17:42:28,568] INFO: Loaded test dataset with 307 samples.
[2025-12-13 17:42:28,579] INFO: loading Word2Vec object from /app/data/w2v.model
[2025-12-13 17:42:28,622] INFO: loading wv recursively from /app/data/w2v.model.wv.* with mmap=None
[2025-12-13 17:42:28,625] INFO: setting ignored attribute cum_table to None
[2025-12-13 17:42:29,158] INFO: Word2Vec lifecycle event {'fname': '/app/data/w2v.model', 'datetime': '2025-12-13T17:42:29.155663', 'gensim': '4.4.0', 'python': '3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}
[2025-12-13 17:42:29,159] INFO: Word2Vec model loaded from /app/data/w2v.model with vocab size 23186 and vector size 64
[2025-12-13 17:42:35,151] INFO: LSTM model loaded from /app/data/lstm_model.pth and ready for evaluation.
[2025-12-13 17:42:35,152] INFO: Starting LSTM evaluation on test set...
[2025-12-13 17:42:35,604] INFO: LSTM Test Loss: 1.3685
[2025-12-13 17:42:35,605] INFO: LSTM Test Accuracy: 43.65%
[2025-12-13 17:42:35,606] INFO: LSTM F1-score (weighted): 0.3853
[2025-12-13 17:42:35,607] INFO: LSTM Confusion Matrix:
[[ 0  0  6  2  5]
 [ 0  0 10 16  9]
 [ 0  0 19 22 24]
 [ 0  0 13 46 47]
 [ 0  0  1 18 69]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 17:42:35,627] INFO: LSTM Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.00      0.00      0.00        35
           2       0.39      0.29      0.33        65
           3       0.44      0.43      0.44       106
           4       0.45      0.78      0.57        88

    accuracy                           0.44       307
   macro avg       0.26      0.30      0.27       307
weighted avg       0.36      0.44      0.39       307

[2025-12-13 17:42:35,629] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 17:42:35,631] INFO: Evaluating TF-IDF + Logistic Regression baseline...
[2025-12-13 17:42:35,667] INFO: Train dataset loaded: 2434 samples.
[2025-12-13 17:42:37,299] INFO: TF-IDF + Logistic Regression trained.
[2025-12-13 17:42:37,341] INFO: TF-IDF + Logistic Regression Test Accuracy: 36.81%
[2025-12-13 17:42:37,342] INFO: TF-IDF + Logistic Regression F1-score (weighted): 0.3350
[2025-12-13 17:42:37,343] INFO: TF-IDF + Logistic Regression Confusion Matrix:
[[ 0  2  4  2  5]
 [ 0  1 16 14  4]
 [ 0  4  9 33 19]
 [ 0  1 24 47 34]
 [ 0  0  8 24 56]]
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", result.shape[0])
[2025-12-13 17:42:37,365] INFO: TF-IDF Classification Report:
              precision    recall  f1-score   support

           0       0.00      0.00      0.00        13
           1       0.12      0.03      0.05        35
           2       0.15      0.14      0.14        65
           3       0.39      0.44      0.42       106
           4       0.47      0.64      0.54        88

    accuracy                           0.37       307
   macro avg       0.23      0.25      0.23       307
weighted avg       0.32      0.37      0.34       307

[2025-12-13 17:42:37,366] INFO: ==== EVALUATION END ====
=== Finished at Sat Dec 13 17:42:38 UTC 2025 ===
